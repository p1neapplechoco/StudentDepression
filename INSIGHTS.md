
# üìä Student Depression Dataset - Ph√¢n t√≠ch & K·∫ø ho·∫°ch Nghi√™n c·ª©u

> **Dataset**: 27,901 b·∫£n ghi √ó 18 features  
> **Target Variable**: `Depression` (0/1 - Binary Classification)

---

## 1. üîç C√°c Insights N·ªïi B·∫≠t (Key Findings)

### üö® "ƒêi·ªÉm n√≥ng" L·ªõp 12 (Class 12)
| Insight | Chi ti·∫øt |
|---------|----------|
| **D·ªØ li·ªáu** | H·ªçc sinh **Class 12** c√≥ t·ª∑ l·ªá tr·∫ßm c·∫£m **70.8%** (cao nh·∫•t) |
| **So s√°nh** | Cao h∆°n trung b√¨nh (58.5%), PhD, MBBS |
| **√ù nghƒ©a** | √Åp l·ª±c thi c·ª≠/chuy·ªÉn c·∫•p > √Åp l·ª±c h·ªçc thu·∫≠t ƒë·∫°i h·ªçc |

### üí§ Gi·∫•c ng·ªß & ƒÇn u·ªëng: "Combo" nguy hi·ªÉm
| Factor | T·ª∑ l·ªá tr·∫ßm c·∫£m |
|--------|----------------|
| Ng·ªß < 5 ti·∫øng/ng√†y | **64.5%** |
| ƒÇn u·ªëng Unhealthy | **70.7%** |
| ƒÇn u·ªëng Healthy | 45.4% |

### üí∏ √Åp l·ª±c T√†i ch√≠nh (Financial Stress)
- **Financial Stress = 5/5** ‚Üí T·ª∑ l·ªá tr·∫ßm c·∫£m **81.3%**
- ƒê√¢y l√† y·∫øu t·ªë "·∫©n" nh∆∞ng c√≥ s·ª©c t√†n ph√° l·ªõn nh·∫•t

### üéì Ngh·ªãch l√Ω ƒêi·ªÉm s·ªë (CGPA Paradox)
| Nh√≥m | CGPA trung b√¨nh |
|------|-----------------|
| Tr·∫ßm c·∫£m | 7.68 |
| Kh√¥ng tr·∫ßm c·∫£m | 7.62 |

‚Üí **High-functioning depression** c√≥ th·ªÉ ƒëang hi·ªán h·ªØu

---

## 2. üéØ Research Questions (C√¢u h·ªèi Nghi√™n c·ª©u)

### RQ1: Hi·ªáu ·ª©ng "Transition Stress"
> **"√Åp l·ª±c giai ƒëo·∫°n chuy·ªÉn ti·∫øp (Class 12 ‚Üí ƒê·∫°i h·ªçc) c√≥ ph·∫£i l√† y·∫øu t·ªë g√¢y tr·∫ßm c·∫£m m·∫°nh h∆°n √°p l·ª±c h·ªçc thu·∫≠t th√¥ng th∆∞·ªùng?"**

**Hypothesis (H1)**: H·ªçc sinh Class 12 c√≥ Academic Pressure cao h∆°n trung b√¨nh, nh∆∞ng y·∫øu t·ªë Sleep v√† Dietary Habits c·ªßa h·ªç t·ªá h∆°n ƒë√°ng k·ªÉ.

**Ki·ªÉm ch·ª©ng**: So s√°nh ph√¢n ph·ªëi c√°c features gi·ªØa nh√≥m Class 12 vs. c√°c Degree kh√°c.

---

### RQ2: Lifestyle Buffer Effect
> **"L·ªëi s·ªëng l√†nh m·∫°nh (Ng·ªß ƒë·ªß + ƒÇn healthy) c√≥ th·ªÉ l√†m gi·∫£m t√°c ƒë·ªông ti√™u c·ª±c c·ªßa Academic Pressure l√™n tr·∫ßm c·∫£m kh√¥ng?"**

**Hypothesis (H2)**: Trong nh√≥m c√≥ Academic Pressure cao (4-5), nh·ªØng ng∆∞·ªùi c√≥ Healthy Lifestyle s·∫Ω c√≥ t·ª∑ l·ªá tr·∫ßm c·∫£m th·∫•p h∆°n ƒë√°ng k·ªÉ so v·ªõi nh√≥m Unhealthy Lifestyle.

**Ki·ªÉm ch·ª©ng**: Ph√¢n t√≠ch interaction effect gi·ªØa `Academic Pressure` √ó `Lifestyle Score` (Sleep + Diet combined).

---

### RQ3: Financial Stress as Hidden Killer
> **"Financial Stress c√≥ ph·∫£i l√† y·∫øu t·ªë d·ª± b√°o tr·∫ßm c·∫£m m·∫°nh nh·∫•t, v∆∞·ª£t tr·ªôi h∆°n c·∫£ Academic Pressure?"**

**Hypothesis (H3)**: Trong m√¥ h√¨nh d·ª± ƒëo√°n, `Financial Stress` s·∫Ω c√≥ feature importance cao nh·∫•t.

**Ki·ªÉm ch·ª©ng**: So s√°nh feature importance t·ª´ nhi·ªÅu m√¥ h√¨nh kh√°c nhau.

---

### RQ4: Risk Profile Clustering
> **"C√≥ th·ªÉ ph√¢n nh√≥m sinh vi√™n th√†nh c√°c 'Risk Profiles' d·ª±a tr√™n ƒë·∫∑c ƒëi·ªÉm c·ªßa h·ªç kh√¥ng?"**

**Hypothesis (H4)**: T·ªìn t·∫°i √≠t nh·∫•t 3-4 clusters v·ªõi t·ª∑ l·ªá tr·∫ßm c·∫£m kh√°c bi·ªát r√µ r·ªát (v√≠ d·ª•: "High Risk", "Moderate Risk", "Low Risk").

**Ki·ªÉm ch·ª©ng**: K-Means/Hierarchical Clustering ‚Üí So s√°nh Depression rate gi·ªØa c√°c clusters.

---

### RQ5: The CGPA Paradox
> **"T·∫°i sao CGPA kh√¥ng kh√°c bi·ªát gi·ªØa nh√≥m tr·∫ßm c·∫£m v√† kh√¥ng tr·∫ßm c·∫£m?"**

**Hypothesis (H5)**: "High-functioning depression" - Sinh vi√™n tr·∫ßm c·∫£m c√≥ th·ªÉ ƒëang "over-compensate" b·∫±ng c√°ch h·ªçc nhi·ªÅu h∆°n, d·∫´n ƒë·∫øn CGPA cao nh∆∞ng hy sinh s·ª©c kh·ªèe tinh th·∫ßn.

**Ki·ªÉm ch·ª©ng**: Ph√¢n t√≠ch correlation gi·ªØa `Work/Study Hours`, `CGPA`, v√† `Depression` trong t·ª´ng subgroup.

---

### RQ6: Family History as Genetic/Environmental Factor
> **"Ti·ªÅn s·ª≠ gia ƒë√¨nh v·ªÅ b·ªánh t√¢m th·∫ßn c√≥ ph·∫£i l√† y·∫øu t·ªë l√†m tƒÉng 'vulnerability' ƒë·ªëi v·ªõi c√°c stressor kh√°c kh√¥ng?"**

**Hypothesis (H6)**: ·ªû nh√≥m c√≥ `Family History = Yes`, m·ªëi quan h·ªá gi·ªØa `Financial Stress` ‚Üí `Depression` s·∫Ω m·∫°nh h∆°n so v·ªõi nh√≥m `Family History = No`.

**Ki·ªÉm ch·ª©ng**: Moderation analysis ho·∫∑c stratified analysis.

---

## 3. üìã K·∫ø ho·∫°ch Preprocessing & Modeling

### Phase 1: Data Cleaning & Preprocessing

#### 1.1 Filter Data
```python
# Ch·ªâ gi·ªØ l·∫°i Students (chi·∫øm 99.9%)
df = df[df['Profession'] == 'Student'].copy()

# Drop c√°c c·ªôt kh√¥ng li√™n quan cho Students
drop_cols = ['id', 'Work Pressure', 'Job Satisfaction', 'Profession', 'City']
df = df.drop(columns=drop_cols, errors='ignore')
```

#### 1.2 Handle Missing Values
| Strategy | √Åp d·ª•ng cho |
|----------|-------------|
| **Mode imputation** | Categorical: `Gender`, `Dietary Habits`, `Degree`, etc. |
| **Median imputation** | Numerical: `Age`, `CGPA`, `Financial Stress`, etc. |
| **Drop rows** | N·∫øu missing > 30% trong m·ªôt row |

#### 1.3 Encode Categorical Variables
| Column | Encoding |
|--------|----------|
| `Sleep Duration` | **Ordinal**: 'Less than 5 hours' < '5-6 hours' < '7-8 hours' < 'More than 8 hours' ‚Üí (0, 1, 2, 3) |
| `Dietary Habits` | **Ordinal**: Unhealthy < Moderate < Healthy ‚Üí (0, 1, 2) |
| `Gender` | **Binary**: Male/Female ‚Üí (0, 1) |
| `Degree` | **One-Hot Encoding** (nhi·ªÅu categories) |
| `Family History of Mental Illness` | **Binary**: No/Yes ‚Üí (0, 1) |
| `Have you ever had suicidal thoughts ?` | **Binary**: No/Yes ‚Üí (0, 1) |

---

### Phase 2: Feature Engineering

#### 2.1 Create Composite Features
```python
# Lifestyle Score (Sleep + Diet combined)
df['Lifestyle_Score'] = df['Sleep_Encoded'] + df['Diet_Encoded']

# Total Stress Score
df['Total_Stress'] = df['Academic Pressure'] + df['Financial Stress']

# Study Efficiency (CGPA per Work/Study Hour)
df['Study_Efficiency'] = df['CGPA'] / (df['Work/Study Hours'] + 1)

# Is_High_Risk_Group (Class 12 binary flag)
df['Is_Class12'] = (df['Degree'] == 'Class 12').astype(int)
```

#### 2.2 Create Interaction Features
```python
# Academic Pressure √ó Lifestyle
df['AcademicPressure_x_Lifestyle'] = df['Academic Pressure'] * df['Lifestyle_Score']

# Financial Stress √ó Family History
df['FinancialStress_x_FamilyHistory'] = df['Financial Stress'] * df['Family_History_Encoded']
```

#### 2.3 Binning / Discretization
```python
# Age Groups
df['Age_Group'] = pd.cut(df['Age'], bins=[0, 20, 25, 30, 100], labels=['Teen', 'Young Adult', 'Adult', 'Mature'])

# CGPA Categories
df['CGPA_Category'] = pd.cut(df['CGPA'], bins=[0, 6, 7.5, 9, 10], labels=['Low', 'Medium', 'High', 'Excellent'])
```

---

### Phase 3: Modeling Strategy

#### 3.1 Train/Test Split
```python
from sklearn.model_selection import train_test_split, StratifiedKFold

# 80/20 split v·ªõi stratification
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# 5-Fold Cross Validation cho model selection
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
```

#### 3.2 Models to Train (By Complexity)

| Level | Model | Purpose |
|-------|-------|---------|
| **Baseline** | Logistic Regression | Interpretability, baseline performance |
| **Tree-based** | Decision Tree | Rule extraction, visualization |
| **Ensemble** | Random Forest | Feature importance, robust performance |
| **Gradient Boosting** | XGBoost / LightGBM | Best performance |
| **Interpretable** | SHAP values | Explain predictions |

#### 3.3 Evaluation Metrics

| Metric | L√Ω do quan tr·ªçng |
|--------|------------------|
| **Accuracy** | Overall performance |
| **Recall (Sensitivity)** | ‚ö†Ô∏è **Quan tr·ªçng nh·∫•t** - Kh√¥ng b·ªè s√≥t sinh vi√™n c√≥ nguy c∆° tr·∫ßm c·∫£m |
| **Precision** | Tr√°nh false alarms |
| **F1-Score** | Balance Precision-Recall |
| **AUC-ROC** | Overall discrimination ability |
| **Confusion Matrix** | Detailed error analysis |

#### 3.4 Addressing Class Imbalance (n·∫øu c√≥)
```python
# Option 1: Class weights
model = LogisticRegression(class_weight='balanced')

# Option 2: SMOTE oversampling
from imblearn.over_sampling import SMOTE
smote = SMOTE(random_state=42)
X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)
```

---

## 4. üìÅ Proposed Project Structure

```
project/
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îî‚îÄ‚îÄ student_depression_dataset.csv    # Raw data
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îú‚îÄ‚îÄ 01_exploration.ipynb              # ‚úÖ EDA (done)
‚îÇ   ‚îú‚îÄ‚îÄ 02_preprocessing.ipynb            # üîú Data cleaning & feature engineering
‚îÇ   ‚îú‚îÄ‚îÄ 03_modeling.ipynb                 # üîú Model training & evaluation
‚îÇ   ‚îî‚îÄ‚îÄ 04_analysis.ipynb                 # üîú Research questions analysis
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ preprocessing.py                  # Preprocessing functions
‚îÇ   ‚îú‚îÄ‚îÄ features.py                       # Feature engineering functions
‚îÇ   ‚îî‚îÄ‚îÄ models.py                         # Model training utilities
‚îú‚îÄ‚îÄ reports/
‚îÇ   ‚îú‚îÄ‚îÄ figures/                          # Saved visualizations
‚îÇ   ‚îî‚îÄ‚îÄ final_report.pdf                  # Final report
‚îî‚îÄ‚îÄ INSIGHTS.md                           # This file
```

---

## 5. üìù Next Steps Checklist

- [ ] **Phase 1: Preprocessing**
  - [ ] Load v√† clean data
  - [ ] Handle missing values
  - [ ] Encode categorical variables
  - [ ] Create notebook `02_preprocessing.ipynb`

- [ ] **Phase 2: Feature Engineering**
  - [ ] Create composite features (Lifestyle Score, Total Stress, etc.)
  - [ ] Create interaction features
  - [ ] Feature selection (correlation analysis, VIF for multicollinearity)

- [ ] **Phase 3: Modeling**
  - [ ] Train baseline (Logistic Regression)
  - [ ] Train tree-based models (Decision Tree, Random Forest)
  - [ ] Train gradient boosting (XGBoost/LightGBM)
  - [ ] Hyperparameter tuning (GridSearchCV/RandomizedSearchCV)
  - [ ] Model comparison & selection

- [ ] **Phase 4: Analysis & Interpretation**
  - [ ] Feature importance analysis
  - [ ] SHAP values interpretation
  - [ ] Answer Research Questions (RQ1-RQ6)
  - [ ] Risk profile clustering

- [ ] **Phase 5: Reporting**
  - [ ] Create visualizations
  - [ ] Write final report
