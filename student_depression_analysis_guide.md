# H∆Ø·ªöNG D·∫™N CHI TI·∫æT: PH√ÇN T√çCH D·ªÆ LI·ªÜU TR·∫¶M C·∫¢M ·ªû SINH VI√äN

## M·ª§C L·ª§C
1. [Gi·ªõi thi·ªáu d·ªØ li·ªáu](#1-gi·ªõi-thi·ªáu-d·ªØ-li·ªáu)
2. [Ph√¢n t√≠ch c∆° b·∫£n v√† t·ªïng quan](#2-ph√¢n-t√≠ch-c∆°-b·∫£n-v√†-t·ªïng-quan)
3. [Ph√¢n t√≠ch c√°c insight trong d·ªØ li·ªáu](#3-ph√¢n-t√≠ch-c√°c-insight-trong-d·ªØ-li·ªáu)
4. [Recommended Research Questions b·ªï sung](#4-recommended-research-questions-b·ªï-sung)

---

# 1. GI·ªöI THI·ªÜU D·ªÆ LI·ªÜU

## 1.1. Setup m√¥i tr∆∞·ªùng v√† Import libraries

```python
# ============================================
# CELL 1: Import Libraries
# ============================================
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from scipy.stats import chi2_contingency, pointbiserialr, spearmanr, pearsonr
import warnings
warnings.filterwarnings('ignore')

# Machine Learning
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import (classification_report, confusion_matrix, 
                            roc_auc_score, roc_curve, accuracy_score,
                            precision_recall_curve, f1_score)
from sklearn.inspection import permutation_importance
import statsmodels.api as sm
from statsmodels.stats.outliers_influence import variance_inflation_factor

# Visualization settings
plt.style.use('seaborn-v0_8-whitegrid')
plt.rcParams['figure.figsize'] = (12, 8)
plt.rcParams['font.size'] = 12
sns.set_palette("husl")

print("‚úÖ Libraries imported successfully!")
```

## 1.2. Kh√°i qu√°t d·ªØ li·ªáu

```python
# ============================================
# CELL 2: Load v√† kh√°i qu√°t d·ªØ li·ªáu
# ============================================
# Load d·ªØ li·ªáu
df = pd.read_csv('student_depression_dataset.csv')

# Markdown text cho ph·∫ßn gi·ªõi thi·ªáu (hi·ªÉn th·ªã trong notebook)
intro_text = """
## 1.1 Kh√°i qu√°t d·ªØ li·ªáu

**T√™n t·∫≠p d·ªØ li·ªáu:** Student Depression Dataset

**M√¥ t·∫£ ng·∫Øn g·ªçn:** 
- T·∫≠p d·ªØ li·ªáu n√†y thu th·∫≠p th√¥ng tin v·ªÅ s·ª©c kh·ªèe tinh th·∫ßn c·ªßa sinh vi√™n, 
  bao g·ªìm c√°c y·∫øu t·ªë nh√¢n kh·∫©u h·ªçc, h·ªçc t·∫≠p, l·ªëi s·ªëng v√† t√¢m l√Ω.
- M·ª•c ti√™u ch√≠nh: Ph√¢n t√≠ch v√† d·ª± ƒëo√°n t√¨nh tr·∫°ng tr·∫ßm c·∫£m ·ªü sinh vi√™n.

**Quy m√¥ d·ªØ li·ªáu:**
- T·ªïng s·ªë b·∫£n ghi: {:,} sinh vi√™n
- T·ªïng s·ªë thu·ªôc t√≠nh: {} c·ªôt
- D·ªØ li·ªáu ƒë∆∞·ª£c thu th·∫≠p t·ª´ c√°c th√†nh ph·ªë ·ªü ·∫§n ƒê·ªô
""".format(len(df), len(df.columns))

print(intro_text)

# Hi·ªÉn th·ªã th√¥ng tin c∆° b·∫£n
print("\n" + "="*60)
print("TH√îNG TIN C∆† B·∫¢N V·ªÄ D·ªÆ LI·ªÜU")
print("="*60)
print(f"\nüìä K√≠ch th∆∞·ªõc: {df.shape[0]:,} d√≤ng x {df.shape[1]} c·ªôt")
print(f"\nüìù C√°c c·ªôt trong d·ªØ li·ªáu:")
for i, col in enumerate(df.columns, 1):
    print(f"   {i:2}. {col}")
```

## 1.3. M√¥ t·∫£ v·ªÅ d·ªØ li·ªáu

```python
# ============================================
# CELL 3: M√¥ t·∫£ chi ti·∫øt t·ª´ng c·ªôt
# ============================================

# Dictionary m√¥ t·∫£ c√°c c·ªôt
column_descriptions = {
    'id': {
        'type': 'Numerical (ID)',
        'description': 'M√£ ƒë·ªãnh danh duy nh·∫•t cho m·ªói sinh vi√™n',
        'values': 'S·ªë nguy√™n d∆∞∆°ng'
    },
    'Gender': {
        'type': 'Categorical',
        'description': 'Gi·ªõi t√≠nh c·ªßa sinh vi√™n',
        'values': 'Male, Female'
    },
    'Age': {
        'type': 'Numerical (Continuous)',
        'description': 'Tu·ªïi c·ªßa sinh vi√™n',
        'values': 'S·ªë nguy√™n (18-35+)'
    },
    'City': {
        'type': 'Categorical',
        'description': 'Th√†nh ph·ªë sinh s·ªëng/h·ªçc t·∫≠p',
        'values': 'C√°c th√†nh ph·ªë ·ªü ·∫§n ƒê·ªô'
    },
    'Profession': {
        'type': 'Categorical',
        'description': 'Ngh·ªÅ nghi·ªáp (trong dataset n√†y ch·ªß y·∫øu l√† Student)',
        'values': 'Student, Working Professional...'
    },
    'Academic Pressure': {
        'type': 'Numerical (Ordinal)',
        'description': 'M·ª©c ƒë·ªô √°p l·ª±c h·ªçc t·∫≠p (t·ª± ƒë√°nh gi√°)',
        'values': '1-5 (1: Th·∫•p nh·∫•t, 5: Cao nh·∫•t)'
    },
    'Work Pressure': {
        'type': 'Numerical (Ordinal)',
        'description': 'M·ª©c ƒë·ªô √°p l·ª±c c√¥ng vi·ªác',
        'values': '0-5 (0 n·∫øu kh√¥ng ƒëi l√†m)'
    },
    'CGPA': {
        'type': 'Numerical (Continuous)',
        'description': 'ƒêi·ªÉm trung b√¨nh t√≠ch l≈©y (Cumulative Grade Point Average)',
        'values': '0.0-10.0'
    },
    'Study Satisfaction': {
        'type': 'Numerical (Ordinal)',
        'description': 'M·ª©c ƒë·ªô h√†i l√≤ng v·ªõi vi·ªác h·ªçc',
        'values': '1-5 (1: R·∫•t kh√¥ng h√†i l√≤ng, 5: R·∫•t h√†i l√≤ng)'
    },
    'Job Satisfaction': {
        'type': 'Numerical (Ordinal)',
        'description': 'M·ª©c ƒë·ªô h√†i l√≤ng v·ªõi c√¥ng vi·ªác',
        'values': '0-5 (0 n·∫øu kh√¥ng c√≥ vi·ªác l√†m)'
    },
    'Sleep Duration': {
        'type': 'Categorical (Ordinal)',
        'description': 'Th·ªùi gian ng·ªß trung b√¨nh m·ªói ng√†y',
        'values': "'Less than 5 hours', '5-6 hours', '7-8 hours', 'More than 8 hours'"
    },
    'Dietary Habits': {
        'type': 'Categorical (Ordinal)',
        'description': 'Th√≥i quen ƒÉn u·ªëng',
        'values': 'Healthy, Moderate, Unhealthy'
    },
    'Degree': {
        'type': 'Categorical',
        'description': 'B·∫±ng c·∫•p ƒëang theo h·ªçc',
        'values': "Class 12, BA, BSc, BE, B.Com, MBA, PhD, ..."
    },
    'Have you ever had suicidal thoughts ?': {
        'type': 'Categorical (Binary)',
        'description': 'ƒê√£ t·ª´ng c√≥ suy nghƒ© t·ª± t·ª≠ ch∆∞a',
        'values': 'Yes, No'
    },
    'Work/Study Hours': {
        'type': 'Numerical (Continuous)',
        'description': 'S·ªë gi·ªù h·ªçc/l√†m vi·ªác m·ªói ng√†y',
        'values': '0-12+'
    },
    'Financial Stress': {
        'type': 'Numerical (Ordinal)',
        'description': 'M·ª©c ƒë·ªô stress v·ªÅ t√†i ch√≠nh',
        'values': '1-5 (1: Th·∫•p nh·∫•t, 5: Cao nh·∫•t)'
    },
    'Family History of Mental Illness': {
        'type': 'Categorical (Binary)',
        'description': 'Gia ƒë√¨nh c√≥ ti·ªÅn s·ª≠ b·ªánh t√¢m th·∫ßn',
        'values': 'Yes, No'
    },
    'Depression': {
        'type': 'Categorical (Binary) - TARGET',
        'description': '‚≠ê BI·∫æN M·ª§C TI√äU: T√¨nh tr·∫°ng tr·∫ßm c·∫£m',
        'values': '0: Kh√¥ng tr·∫ßm c·∫£m, 1: C√≥ tr·∫ßm c·∫£m'
    }
}

# Hi·ªÉn th·ªã b·∫£ng m√¥ t·∫£
print("\n" + "="*80)
print("M√î T·∫¢ CHI TI·∫æT C√ÅC THU·ªòC T√çNH")
print("="*80)

desc_data = []
for col, info in column_descriptions.items():
    desc_data.append({
        'T√™n c·ªôt': col,
        'Lo·∫°i d·ªØ li·ªáu': info['type'],
        'M√¥ t·∫£': info['description'],
        'Gi√° tr·ªã': info['values']
    })

desc_df = pd.DataFrame(desc_data)
display(desc_df.style.set_properties(**{'text-align': 'left'}))

# Ph√¢n lo·∫°i bi·∫øn
print("\n" + "="*60)
print("PH√ÇN LO·∫†I BI·∫æN")
print("="*60)

numerical_vars = ['Age', 'CGPA', 'Work/Study Hours', 'Academic Pressure', 
                  'Work Pressure', 'Study Satisfaction', 'Job Satisfaction', 
                  'Financial Stress']
categorical_vars = ['Gender', 'City', 'Profession', 'Sleep Duration', 
                   'Dietary Habits', 'Degree', 
                   'Have you ever had suicidal thoughts ?',
                   'Family History of Mental Illness']
target_var = 'Depression'

print(f"\nüìä Bi·∫øn s·ªë (Numerical): {len(numerical_vars)} bi·∫øn")
for v in numerical_vars:
    print(f"   - {v}")

print(f"\nüìã Bi·∫øn ph√¢n lo·∫°i (Categorical): {len(categorical_vars)} bi·∫øn")
for v in categorical_vars:
    print(f"   - {v}")

print(f"\nüéØ Bi·∫øn m·ª•c ti√™u (Target): {target_var}")
```

## 1.4. Ngu·ªìn d·ªØ li·ªáu

```python
# ============================================
# CELL 4: Th√¥ng tin ngu·ªìn d·ªØ li·ªáu
# ============================================

source_info = """
## 1.3 Ngu·ªìn d·ªØ li·ªáu

### Th√¥ng tin ch√≠nh
- **N·ªÅn t·∫£ng:** Kaggle
- **Link:** https://www.kaggle.com/datasets/adilshamim8/student-depression-dataset/data
- **T√°c gi·∫£:** Adil Shamim
- **Usability Score:** 10.0/10

### B·ªëi c·∫£nh thu th·∫≠p
- D·ªØ li·ªáu ƒë∆∞·ª£c thu th·∫≠p t·ª´ sinh vi√™n t·∫°i c√°c th√†nh ph·ªë ·ªü ·∫§n ƒê·ªô
- Bao g·ªìm nhi·ªÅu tr∆∞·ªùng ƒë·∫°i h·ªçc v√† c·∫•p b·∫≠c h·ªçc kh√°c nhau
- D·ªØ li·ªáu t·ª± b√°o c√°o (self-reported) th√¥ng qua kh·∫£o s√°t

### ƒê·∫∑c ƒëi·ªÉm n·ªïi b·∫≠t c·ªßa dataset
1. **Quy m√¥ l·ªõn:** ~27,901 b·∫£n ghi - ƒë·ªß l·ªõn cho ph√¢n t√≠ch th·ªëng k√™
2. **ƒêa chi·ªÅu:** Bao g·ªìm y·∫øu t·ªë nh√¢n kh·∫©u h·ªçc, h·ªçc t·∫≠p, l·ªëi s·ªëng, t√¢m l√Ω
3. **Th·ª±c ti·ªÖn:** D·ªØ li·ªáu t·ª´ m√¥i tr∆∞·ªùng h·ªçc t·∫≠p th·ª±c t·∫ø
4. **C√¢n b·∫±ng h·ª£p l√Ω:** T·ª∑ l·ªá tr·∫ßm c·∫£m/kh√¥ng tr·∫ßm c·∫£m kh√¥ng qu√° ch√™nh l·ªách

### L∆∞u √Ω v·ªÅ d·ªØ li·ªáu
- D·ªØ li·ªáu t·ª± b√°o c√°o c√≥ th·ªÉ c√≥ bias
- Kh√¥ng c√≥ th√¥ng tin v·ªÅ ph∆∞∆°ng ph√°p ch·∫©n ƒëo√°n tr·∫ßm c·∫£m c·ª• th·ªÉ
- C·∫ßn th·∫≠n tr·ªçng khi t·ªïng qu√°t h√≥a k·∫øt qu·∫£
"""
print(source_info)
```

## 1.5. L√Ω do ch·ªçn t·∫≠p d·ªØ li·ªáu

```python
# ============================================
# CELL 5: L√Ω do ch·ªçn t·∫≠p d·ªØ li·ªáu
# ============================================

reasons_text = """
## 1.4 L√Ω do ch·ªçn t·∫≠p d·ªØ li·ªáu

### 1. T√≠nh th·ªùi s·ª± v√† c·∫•p thi·∫øt
- **Kh·ªßng ho·∫£ng s·ª©c kh·ªèe t√¢m th·∫ßn to√†n c·∫ßu:** WHO x√°c ƒë·ªãnh tr·∫ßm c·∫£m l√† m·ªôt trong nh·ªØng 
  nguy√™n nh√¢n h√†ng ƒë·∫ßu g√¢y ra r·ªëi lo·∫°n s·ª©c kh·ªèe t√¢m th·∫ßn ·ªü nh√≥m tu·ªïi sinh s·∫£n
- **·∫¢nh h∆∞·ªüng ƒë·∫øn sinh vi√™n:** √Åp l·ª±c h·ªçc t·∫≠p, ƒë·ªãnh h∆∞·ªõng ngh·ªÅ nghi·ªáp, v√† chuy·ªÉn ƒë·ªïi 
  cu·ªôc s·ªëng ƒë·ªÅu g√≥p ph·∫ßn v√†o nguy c∆° tr·∫ßm c·∫£m

### 2. Gi√° tr·ªã nghi√™n c·ª©u
- **Nhi·ªÅu chi·ªÅu d·ªØ li·ªáu:** Cho ph√©p ph√¢n t√≠ch ƒëa y·∫øu t·ªë
- **Bi·∫øn m·ª•c ti√™u r√µ r√†ng:** Binary classification - d·ªÖ d√†ng ƒë√°nh gi√° m√¥ h√¨nh
- **Quy m√¥ ƒë·ªß l·ªõn:** >27,000 m·∫´u ƒë·∫£m b·∫£o ƒë·ªô tin c·∫≠y th·ªëng k√™

### 3. ·ª®ng d·ª•ng th·ª±c ti·ªÖn
- **C·∫£nh b√°o s·ªõm:** C√≥ th·ªÉ x√¢y d·ª±ng h·ªá th·ªëng ph√°t hi·ªán s·ªõm sinh vi√™n c√≥ nguy c∆°
- **ƒê·ªãnh h∆∞·ªõng can thi·ªáp:** X√°c ƒë·ªãnh c√°c y·∫øu t·ªë r·ªßi ro ƒë·ªÉ ƒë·ªÅ xu·∫•t bi·ªán ph√°p
- **H·ªó tr·ª£ ch√≠nh s√°ch:** Cung c·∫•p insights cho c√°c c∆° s·ªü gi√°o d·ª•c

### 4. T√≠nh kh·∫£ thi k·ªπ thu·∫≠t
- **D·ªØ li·ªáu s·∫°ch:** √çt missing values
- **C·∫•u tr√∫c r√µ r√†ng:** D·ªÖ preprocessing
- **Ph√π h·ª£p ML:** C√≥ th·ªÉ √°p d·ª•ng nhi·ªÅu thu·∫≠t to√°n kh√°c nhau

### C√¢u h·ªèi nghi√™n c·ª©u ch√≠nh
1. Y·∫øu t·ªë n√†o ·∫£nh h∆∞·ªüng m·∫°nh nh·∫•t ƒë·∫øn tr·∫ßm c·∫£m ·ªü sinh vi√™n?
2. C√≥ th·ªÉ d·ª± ƒëo√°n tr·∫ßm c·∫£m d·ª±a tr√™n c√°c ƒë·∫∑c ƒëi·ªÉm c√≥ s·∫µn kh√¥ng?
3. M·ªëi quan h·ªá gi·ªØa √°p l·ª±c h·ªçc t·∫≠p v√† c√°c y·∫øu t·ªë kh√°c nh∆∞ th·∫ø n√†o?
"""
print(reasons_text)
```

---

# 2. PH√ÇN T√çCH C∆† B·∫¢N V√Ä T·ªîNG QUAN V·ªÄ T·∫¨P D·ªÆ LI·ªÜU

## 2.1. Data Overview

```python
# ============================================
# CELL 6: T·ªïng quan d·ªØ li·ªáu
# ============================================

print("="*70)
print("2. PH√ÇN T√çCH C∆† B·∫¢N V√Ä T·ªîNG QUAN")
print("="*70)

# 2.1 Xem m·∫´u d·ªØ li·ªáu
print("\nüìä 2.1 M·∫´u d·ªØ li·ªáu ƒë·∫ßu ti√™n:")
display(df.head(10))

print("\nüìä 2.2 Th√¥ng tin t·ªïng quan:")
print(df.info())

print("\nüìä 2.3 Th·ªëng k√™ m√¥ t·∫£ cho bi·∫øn s·ªë:")
display(df.describe())

print("\nüìä 2.4 Th·ªëng k√™ m√¥ t·∫£ cho bi·∫øn ph√¢n lo·∫°i:")
display(df.describe(include='object'))
```

## 2.2. Missing Values Analysis

```python
# ============================================
# CELL 7: Ph√¢n t√≠ch Missing Values
# ============================================

print("\n" + "="*60)
print("2.5 PH√ÇN T√çCH MISSING VALUES")
print("="*60)

# T√≠nh missing values
missing_data = pd.DataFrame({
    'Missing Count': df.isnull().sum(),
    'Missing %': (df.isnull().sum() / len(df) * 100).round(2)
})
missing_data = missing_data[missing_data['Missing Count'] > 0].sort_values('Missing %', ascending=False)

if len(missing_data) > 0:
    print("\n‚ö†Ô∏è C√°c c·ªôt c√≥ gi√° tr·ªã thi·∫øu:")
    display(missing_data)
    
    # Visualization
    fig, ax = plt.subplots(figsize=(10, 6))
    missing_data['Missing %'].plot(kind='barh', color='coral', ax=ax)
    ax.set_xlabel('T·ª∑ l·ªá Missing (%)')
    ax.set_title('T·ª∑ l·ªá Missing Values theo c·ªôt')
    plt.tight_layout()
    plt.show()
else:
    print("\n‚úÖ Kh√¥ng c√≥ missing values trong d·ªØ li·ªáu!")

# Ki·ªÉm tra duplicate
duplicates = df.duplicated().sum()
print(f"\nüìã S·ªë b·∫£n ghi tr√πng l·∫∑p: {duplicates}")
```

## 2.3. Data Type Verification & Cleaning

```python
# ============================================
# CELL 8: Ki·ªÉm tra v√† x·ª≠ l√Ω data types
# ============================================

print("\n" + "="*60)
print("2.6 KI·ªÇM TRA V√Ä X·ª¨ L√ù D·ªÆ LI·ªÜU")
print("="*60)

# Ki·ªÉm tra gi√° tr·ªã unique c·ªßa c√°c c·ªôt categorical
categorical_cols = ['Gender', 'City', 'Profession', 'Sleep Duration', 
                   'Dietary Habits', 'Degree', 
                   'Have you ever had suicidal thoughts ?',
                   'Family History of Mental Illness']

print("\nüìã Gi√° tr·ªã unique c·ªßa c√°c bi·∫øn categorical:")
for col in categorical_cols:
    if col in df.columns:
        print(f"\n{col}:")
        print(f"   Unique values: {df[col].nunique()}")
        print(f"   Values: {df[col].unique()[:10]}...")  # Hi·ªÉn th·ªã t·ªëi ƒëa 10

# Ki·ªÉm tra gi√° tr·ªã c·ªßa bi·∫øn target
print(f"\nüéØ Ph√¢n ph·ªëi bi·∫øn m·ª•c ti√™u (Depression):")
print(df['Depression'].value_counts())
print(f"\nT·ª∑ l·ªá: {df['Depression'].value_counts(normalize=True).round(3)}")
```

## 2.4. Data Preprocessing

```python
# ============================================
# CELL 9: Data Preprocessing
# ============================================

print("\n" + "="*60)
print("2.7 DATA PREPROCESSING")
print("="*60)

# T·∫°o copy ƒë·ªÉ x·ª≠ l√Ω
df_clean = df.copy()

# 1. X·ª≠ l√Ω c·ªôt Sleep Duration - lo·∫°i b·ªè d·∫•u nh√°y ƒë∆°n
if 'Sleep Duration' in df_clean.columns:
    df_clean['Sleep Duration'] = df_clean['Sleep Duration'].str.replace("'", "")
    print("‚úÖ ƒê√£ clean c·ªôt Sleep Duration")

# 2. T·∫°o c·ªôt Sleep Duration d·∫°ng s·ªë (ordinal encoding)
sleep_mapping = {
    'Less than 5 hours': 1,
    '5-6 hours': 2,
    '7-8 hours': 3,
    'More than 8 hours': 4
}
df_clean['Sleep_Hours_Encoded'] = df_clean['Sleep Duration'].map(sleep_mapping)

# 3. T·∫°o c·ªôt Dietary Habits d·∫°ng s·ªë
diet_mapping = {
    'Unhealthy': 1,
    'Moderate': 2,
    'Healthy': 3
}
df_clean['Diet_Encoded'] = df_clean['Dietary Habits'].map(diet_mapping)

# 4. Encode binary columns
df_clean['Suicidal_Thoughts'] = df_clean['Have you ever had suicidal thoughts ?'].map({'Yes': 1, 'No': 0})
df_clean['Family_History'] = df_clean['Family History of Mental Illness'].map({'Yes': 1, 'No': 0})
df_clean['Gender_Encoded'] = df_clean['Gender'].map({'Male': 0, 'Female': 1})

# 5. Ki·ªÉm tra v√† x·ª≠ l√Ω outliers trong CGPA
print(f"\nüìä CGPA Statistics:")
print(f"   Min: {df_clean['CGPA'].min()}")
print(f"   Max: {df_clean['CGPA'].max()}")
print(f"   Mean: {df_clean['CGPA'].mean():.2f}")

# 6. X√≥a c·ªôt id (kh√¥ng c·∫ßn thi·∫øt cho ph√¢n t√≠ch)
if 'id' in df_clean.columns:
    df_clean = df_clean.drop('id', axis=1)
    print("‚úÖ ƒê√£ x√≥a c·ªôt 'id'")

print("\n‚úÖ Preprocessing ho√†n t·∫•t!")
print(f"   Shape sau preprocessing: {df_clean.shape}")

# Hi·ªÉn th·ªã c√°c c·ªôt m·ªõi
print("\nüìã C√°c c·ªôt m·ªõi ƒë∆∞·ª£c t·∫°o:")
new_cols = ['Sleep_Hours_Encoded', 'Diet_Encoded', 'Suicidal_Thoughts', 
            'Family_History', 'Gender_Encoded']
for col in new_cols:
    print(f"   - {col}")
```

## 2.5. Distribution Analysis

```python
# ============================================
# CELL 10: Ph√¢n t√≠ch ph√¢n ph·ªëi
# ============================================

print("\n" + "="*60)
print("2.8 PH√ÇN T√çCH PH√ÇN PH·ªêI D·ªÆ LI·ªÜU")
print("="*60)

# Numerical variables
numerical_cols = ['Age', 'CGPA', 'Academic Pressure', 'Work Pressure',
                  'Study Satisfaction', 'Job Satisfaction', 'Work/Study Hours',
                  'Financial Stress']

fig, axes = plt.subplots(3, 3, figsize=(15, 12))
axes = axes.flatten()

for idx, col in enumerate(numerical_cols):
    if col in df_clean.columns:
        ax = axes[idx]
        
        # Histogram v·ªõi KDE
        df_clean[col].hist(bins=30, ax=ax, alpha=0.7, color='steelblue', edgecolor='black')
        ax.set_title(f'Distribution of {col}', fontsize=11)
        ax.set_xlabel(col)
        ax.set_ylabel('Frequency')
        
        # Th√™m th·ªëng k√™
        mean_val = df_clean[col].mean()
        median_val = df_clean[col].median()
        ax.axvline(mean_val, color='red', linestyle='--', label=f'Mean: {mean_val:.2f}')
        ax.axvline(median_val, color='green', linestyle='--', label=f'Median: {median_val:.2f}')
        ax.legend(fontsize=8)

# ·∫®n subplot th·ª´a
for idx in range(len(numerical_cols), len(axes)):
    axes[idx].axis('off')

plt.suptitle('Ph√¢n ph·ªëi c√°c bi·∫øn s·ªë', fontsize=14, fontweight='bold')
plt.tight_layout()
plt.show()

# Categorical variables
print("\nüìä Ph√¢n ph·ªëi c√°c bi·∫øn categorical:")

fig, axes = plt.subplots(2, 4, figsize=(16, 8))
axes = axes.flatten()

categorical_for_plot = ['Gender', 'Sleep Duration', 'Dietary Habits', 
                        'Have you ever had suicidal thoughts ?',
                        'Family History of Mental Illness', 'Depression']

for idx, col in enumerate(categorical_for_plot):
    if col in df_clean.columns:
        ax = axes[idx]
        value_counts = df_clean[col].value_counts()
        
        bars = ax.bar(range(len(value_counts)), value_counts.values, 
                      color=plt.cm.Set3(np.linspace(0, 1, len(value_counts))))
        ax.set_xticks(range(len(value_counts)))
        ax.set_xticklabels(value_counts.index, rotation=45, ha='right', fontsize=9)
        ax.set_title(col, fontsize=11)
        ax.set_ylabel('Count')
        
        # Th√™m s·ªë li·ªáu tr√™n c·ªôt
        for bar, val in zip(bars, value_counts.values):
            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 50,
                   f'{val:,}', ha='center', va='bottom', fontsize=8)

# ·∫®n subplot th·ª´a
for idx in range(len(categorical_for_plot), len(axes)):
    axes[idx].axis('off')

plt.suptitle('Ph√¢n ph·ªëi c√°c bi·∫øn ph√¢n lo·∫°i', fontsize=14, fontweight='bold')
plt.tight_layout()
plt.show()
```

## 2.6. Correlation Analysis

```python
# ============================================
# CELL 11: Correlation Analysis
# ============================================

print("\n" + "="*60)
print("2.9 PH√ÇN T√çCH T∆Ø∆†NG QUAN")
print("="*60)

# Ch·ªçn c√°c bi·∫øn s·ªë cho correlation matrix
corr_cols = ['Age', 'Academic Pressure', 'Work Pressure', 'CGPA',
             'Study Satisfaction', 'Job Satisfaction', 'Work/Study Hours',
             'Financial Stress', 'Sleep_Hours_Encoded', 'Diet_Encoded',
             'Suicidal_Thoughts', 'Family_History', 'Depression']

corr_matrix = df_clean[corr_cols].corr()

# Heatmap
fig, ax = plt.subplots(figsize=(14, 10))
mask = np.triu(np.ones_like(corr_matrix, dtype=bool))
sns.heatmap(corr_matrix, mask=mask, annot=True, cmap='RdBu_r', center=0,
            fmt='.2f', square=True, linewidths=0.5, ax=ax,
            annot_kws={'size': 9})
ax.set_title('Ma tr·∫≠n t∆∞∆°ng quan gi·ªØa c√°c bi·∫øn', fontsize=14, fontweight='bold')
plt.tight_layout()
plt.show()

# Correlation v·ªõi bi·∫øn target
print("\nüìä T∆∞∆°ng quan v·ªõi bi·∫øn Depression (x·∫øp theo |r|):")
target_corr = corr_matrix['Depression'].drop('Depression').abs().sort_values(ascending=False)
target_corr_df = pd.DataFrame({
    'Variable': target_corr.index,
    'Correlation': corr_matrix['Depression'].drop('Depression')[target_corr.index].values,
    '|Correlation|': target_corr.values
})
display(target_corr_df)

# Bar chart
fig, ax = plt.subplots(figsize=(10, 6))
colors = ['green' if x > 0 else 'red' for x in target_corr_df['Correlation']]
ax.barh(target_corr_df['Variable'], target_corr_df['Correlation'], color=colors, alpha=0.7)
ax.axvline(0, color='black', linewidth=0.5)
ax.set_xlabel('Correlation coefficient')
ax.set_title('T∆∞∆°ng quan c·ªßa c√°c bi·∫øn v·ªõi Depression', fontsize=12)
plt.tight_layout()
plt.show()
```

---

# 3. PH√ÇN T√çCH C√ÅC INSIGHT TRONG D·ªÆ LI·ªÜU

## 3.1. T·ªîNG TH·ªÇ

### Q1 & Q2: T·ª∑ l·ªá tr·∫ßm c·∫£m v√† ph√¢n b·ªë

```python
# ============================================
# CELL 12: Q1 & Q2 - T·ª∑ l·ªá v√† ph√¢n b·ªë tr·∫ßm c·∫£m
# ============================================

print("="*70)
print("3.1 T·ªîNG TH·ªÇ")
print("="*70)
print("\n" + "-"*50)
print("Q1 & Q2: T·ª∑ l·ªá tr·∫ßm c·∫£m trong sinh vi√™n l√† bao nhi√™u v√† ph√¢n b·ªë th·∫ø n√†o?")
print("-"*50)

# T·ª∑ l·ªá t·ªïng th·ªÉ
depression_counts = df_clean['Depression'].value_counts()
depression_pct = df_clean['Depression'].value_counts(normalize=True) * 100

print(f"\nüìä T·ª∑ l·ªá tr·∫ßm c·∫£m t·ªïng th·ªÉ:")
print(f"   - Kh√¥ng tr·∫ßm c·∫£m (0): {depression_counts[0]:,} ({depression_pct[0]:.1f}%)")
print(f"   - C√≥ tr·∫ßm c·∫£m (1): {depression_counts[1]:,} ({depression_pct[1]:.1f}%)")

# Visualization - Pie chart v√† Bar chart
fig, axes = plt.subplots(1, 3, figsize=(16, 5))

# Pie chart
colors = ['#2ecc71', '#e74c3c']
explode = (0, 0.05)
axes[0].pie(depression_pct, labels=['Kh√¥ng tr·∫ßm c·∫£m', 'C√≥ tr·∫ßm c·∫£m'],
           autopct='%1.1f%%', explode=explode, colors=colors,
           shadow=True, startangle=90)
axes[0].set_title('T·ª∑ l·ªá tr·∫ßm c·∫£m t·ªïng th·ªÉ', fontsize=12, fontweight='bold')

# Ph√¢n b·ªë theo gi·ªõi t√≠nh
gender_dep = pd.crosstab(df_clean['Gender'], df_clean['Depression'], normalize='index') * 100
gender_dep.plot(kind='bar', ax=axes[1], color=colors, edgecolor='black')
axes[1].set_title('T·ª∑ l·ªá tr·∫ßm c·∫£m theo Gi·ªõi t√≠nh', fontsize=12, fontweight='bold')
axes[1].set_xlabel('Gi·ªõi t√≠nh')
axes[1].set_ylabel('T·ª∑ l·ªá (%)')
axes[1].legend(['Kh√¥ng tr·∫ßm c·∫£m', 'C√≥ tr·∫ßm c·∫£m'], loc='upper right')
axes[1].tick_params(axis='x', rotation=0)
for container in axes[1].containers:
    axes[1].bar_label(container, fmt='%.1f%%', fontsize=8)

# Ph√¢n b·ªë theo ƒë·ªô tu·ªïi
age_bins = [17, 20, 23, 26, 30, 35, 40]
age_labels = ['18-20', '21-23', '24-26', '27-30', '31-35', '36+']
df_clean['Age_Group'] = pd.cut(df_clean['Age'], bins=age_bins, labels=age_labels)

age_dep = df_clean.groupby('Age_Group')['Depression'].mean() * 100
axes[2].bar(age_dep.index, age_dep.values, color='coral', edgecolor='black')
axes[2].set_title('T·ª∑ l·ªá tr·∫ßm c·∫£m theo Nh√≥m tu·ªïi', fontsize=12, fontweight='bold')
axes[2].set_xlabel('Nh√≥m tu·ªïi')
axes[2].set_ylabel('T·ª∑ l·ªá tr·∫ßm c·∫£m (%)')
for i, v in enumerate(age_dep.values):
    axes[2].text(i, v + 1, f'{v:.1f}%', ha='center', fontsize=9)

plt.tight_layout()
plt.show()

# Chi-square test for gender
print("\nüìà Ki·ªÉm ƒë·ªãnh Chi-square: M·ªëi quan h·ªá gi·ªØa Gi·ªõi t√≠nh v√† Tr·∫ßm c·∫£m")
contingency_table = pd.crosstab(df_clean['Gender'], df_clean['Depression'])
chi2, p_value, dof, expected = chi2_contingency(contingency_table)
print(f"   Chi-square statistic: {chi2:.4f}")
print(f"   p-value: {p_value:.4f}")
print(f"   K·∫øt lu·∫≠n: {'C√≥ m·ªëi quan h·ªá c√≥ √Ω nghƒ©a th·ªëng k√™' if p_value < 0.05 else 'Kh√¥ng c√≥ m·ªëi quan h·ªá c√≥ √Ω nghƒ©a th·ªëng k√™'}")

# Summary statistics by depression status
print("\nüìä Th·ªëng k√™ m√¥ t·∫£ theo t√¨nh tr·∫°ng tr·∫ßm c·∫£m:")
summary_by_dep = df_clean.groupby('Depression')[['Age', 'Academic Pressure', 'CGPA', 
                                                   'Study Satisfaction', 'Financial Stress',
                                                   'Work/Study Hours']].mean().round(2)
summary_by_dep.index = ['Kh√¥ng tr·∫ßm c·∫£m', 'C√≥ tr·∫ßm c·∫£m']
display(summary_by_dep)
```

---

## 3.2. √ÅP L·ª∞C H·ªåC T·∫¨P V√Ä H√ÄI L√íNG

### Q3: Academic Pressure c√≥ th·ª±c s·ª± li√™n quan m·∫°nh ƒë·∫øn tr·∫ßm c·∫£m?

```python
# ============================================
# CELL 13: Q3 - Academic Pressure v√† Tr·∫ßm c·∫£m
# ============================================

print("\n" + "="*70)
print("3.2 √ÅP L·ª∞C H·ªåC T·∫¨P V√Ä H√ÄI L√íNG")
print("="*70)
print("\n" + "-"*50)
print("Q3: Academic Pressure c√≥ th·ª±c s·ª± li√™n quan m·∫°nh ƒë·∫øn tr·∫ßm c·∫£m?")
print("-"*50)

# 1. Point-biserial correlation
r_pb, p_pb = pointbiserialr(df_clean['Depression'], df_clean['Academic Pressure'])
print(f"\nüìä Point-biserial correlation:")
print(f"   r = {r_pb:.4f}, p-value = {p_pb:.4e}")
print(f"   Gi·∫£i th√≠ch: {'T∆∞∆°ng quan m·∫°nh' if abs(r_pb) > 0.3 else 'T∆∞∆°ng quan trung b√¨nh' if abs(r_pb) > 0.2 else 'T∆∞∆°ng quan y·∫øu'}")

# 2. T·ª∑ l·ªá tr·∫ßm c·∫£m theo m·ª©c Academic Pressure
pressure_dep = df_clean.groupby('Academic Pressure').agg({
    'Depression': ['mean', 'count']
}).round(3)
pressure_dep.columns = ['Depression Rate', 'Count']
pressure_dep['Depression Rate %'] = (pressure_dep['Depression Rate'] * 100).round(1)

print("\nüìä T·ª∑ l·ªá tr·∫ßm c·∫£m theo m·ª©c Academic Pressure:")
display(pressure_dep)

# 3. Visualization
fig, axes = plt.subplots(1, 3, figsize=(16, 5))

# Bar chart - Depression rate by Academic Pressure
ax1 = axes[0]
colors_grad = plt.cm.Reds(np.linspace(0.3, 0.9, 5))
bars = ax1.bar(pressure_dep.index, pressure_dep['Depression Rate %'], 
               color=colors_grad, edgecolor='black')
ax1.set_xlabel('Academic Pressure Level')
ax1.set_ylabel('Depression Rate (%)')
ax1.set_title('T·ª∑ l·ªá tr·∫ßm c·∫£m theo m·ª©c Academic Pressure', fontsize=11, fontweight='bold')
for bar in bars:
    height = bar.get_height()
    ax1.text(bar.get_x() + bar.get_width()/2., height + 1,
            f'{height:.1f}%', ha='center', va='bottom', fontsize=10)

# Box plot
ax2 = axes[1]
df_clean.boxplot(column='Academic Pressure', by='Depression', ax=ax2)
ax2.set_xlabel('Depression Status')
ax2.set_ylabel('Academic Pressure')
ax2.set_title('Ph√¢n ph·ªëi Academic Pressure theo Depression', fontsize=11, fontweight='bold')
ax2.set_xticklabels(['Kh√¥ng tr·∫ßm c·∫£m (0)', 'C√≥ tr·∫ßm c·∫£m (1)'])
plt.suptitle('')

# Violin plot
ax3 = axes[2]
parts = ax3.violinplot([df_clean[df_clean['Depression']==0]['Academic Pressure'].dropna(),
                        df_clean[df_clean['Depression']==1]['Academic Pressure'].dropna()],
                       positions=[0, 1], showmeans=True, showmedians=True)
ax3.set_xticks([0, 1])
ax3.set_xticklabels(['Kh√¥ng tr·∫ßm c·∫£m', 'C√≥ tr·∫ßm c·∫£m'])
ax3.set_ylabel('Academic Pressure')
ax3.set_title('Violin Plot: Academic Pressure by Depression', fontsize=11, fontweight='bold')

plt.tight_layout()
plt.show()

# 4. Odds Ratio t√≠nh to√°n
print("\nüìä Odds Ratio Analysis:")
# High pressure (4-5) vs Low pressure (1-2)
high_pressure = df_clean[df_clean['Academic Pressure'] >= 4]
low_pressure = df_clean[df_clean['Academic Pressure'] <= 2]

high_dep = high_pressure['Depression'].sum()
high_no_dep = len(high_pressure) - high_dep
low_dep = low_pressure['Depression'].sum()
low_no_dep = len(low_pressure) - low_dep

odds_high = high_dep / high_no_dep if high_no_dep > 0 else np.inf
odds_low = low_dep / low_no_dep if low_no_dep > 0 else np.inf
odds_ratio = odds_high / odds_low if odds_low > 0 else np.inf

print(f"   Odds (High Pressure): {odds_high:.4f}")
print(f"   Odds (Low Pressure): {odds_low:.4f}")
print(f"   Odds Ratio: {odds_ratio:.2f}")
print(f"   ‚û§ Sinh vi√™n c√≥ √°p l·ª±c cao (4-5) c√≥ nguy c∆° tr·∫ßm c·∫£m cao g·∫•p {odds_ratio:.1f} l·∫ßn so v·ªõi sinh vi√™n c√≥ √°p l·ª±c th·∫•p (1-2)")
```

### Q4: Study Satisfaction c√≥ l√†m gi·∫£m t√°c ƒë·ªông ti√™u c·ª±c c·ªßa Academic Pressure kh√¥ng?

```python
# ============================================
# CELL 14: Q4 - Study Satisfaction ƒëi·ªÅu ti·∫øt Academic Pressure
# ============================================

print("\n" + "-"*50)
print("Q4: Study Satisfaction c√≥ l√†m gi·∫£m t√°c ƒë·ªông ti√™u c·ª±c c·ªßa Academic Pressure kh√¥ng?")
print("-"*50)

# 1. Ph√¢n t√≠ch interaction effect
# Chia nh√≥m Study Satisfaction
df_clean['Satisfaction_Group'] = pd.cut(df_clean['Study Satisfaction'], 
                                        bins=[0, 2, 3, 5], 
                                        labels=['Low (1-2)', 'Medium (3)', 'High (4-5)'])

# T·∫°o b·∫£ng cross-tab
interaction_table = df_clean.groupby(['Academic Pressure', 'Satisfaction_Group'])['Depression'].mean() * 100
interaction_pivot = interaction_table.unstack()

print("\nüìä T·ª∑ l·ªá tr·∫ßm c·∫£m (%) theo Academic Pressure v√† Study Satisfaction:")
display(interaction_pivot.round(1))

# 2. Visualization - Interaction Plot
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Line plot
ax1 = axes[0]
for col in interaction_pivot.columns:
    ax1.plot(interaction_pivot.index, interaction_pivot[col], marker='o', 
             linewidth=2, markersize=8, label=col)
ax1.set_xlabel('Academic Pressure Level')
ax1.set_ylabel('Depression Rate (%)')
ax1.set_title('Interaction: Academic Pressure √ó Study Satisfaction', fontsize=11, fontweight='bold')
ax1.legend(title='Study Satisfaction')
ax1.grid(True, alpha=0.3)

# Heatmap
ax2 = axes[1]
sns.heatmap(interaction_pivot, annot=True, fmt='.1f', cmap='YlOrRd', ax=ax2, cbar_kws={'label': 'Depression Rate (%)'})
ax2.set_title('Heatmap: Depression Rate (%)', fontsize=11, fontweight='bold')
ax2.set_xlabel('Study Satisfaction')
ax2.set_ylabel('Academic Pressure')

plt.tight_layout()
plt.show()

# 3. Logistic Regression ƒë·ªÉ ki·ªÉm tra interaction
print("\nüìà Logistic Regression v·ªõi Interaction Term:")

# Chu·∫©n b·ªã d·ªØ li·ªáu
df_model = df_clean[['Academic Pressure', 'Study Satisfaction', 'Depression']].dropna()
df_model['AP_SS_Interaction'] = df_model['Academic Pressure'] * df_model['Study Satisfaction']

X = df_model[['Academic Pressure', 'Study Satisfaction', 'AP_SS_Interaction']]
X = sm.add_constant(X)
y = df_model['Depression']

# Fit model
model = sm.Logit(y, X).fit(disp=0)
print(model.summary2().tables[1])

# Gi·∫£i th√≠ch
print("\nüìù Gi·∫£i th√≠ch:")
print("   - N·∫øu coefficient c·ªßa Interaction term c√≥ √Ω nghƒ©a th·ªëng k√™ (p < 0.05),")
print("     th√¨ Study Satisfaction c√≥ t√°c ƒë·ªông ƒëi·ªÅu ti·∫øt l√™n m·ªëi quan h·ªá")
print("     gi·ªØa Academic Pressure v√† Depression.")
print(f"   - Interaction coefficient: {model.params['AP_SS_Interaction']:.4f}")
print(f"   - p-value: {model.pvalues['AP_SS_Interaction']:.4f}")

if model.pvalues['AP_SS_Interaction'] < 0.05:
    if model.params['AP_SS_Interaction'] < 0:
        print("   ‚û§ K·∫æT LU·∫¨N: Study Satisfaction C√ì l√†m gi·∫£m t√°c ƒë·ªông ti√™u c·ª±c c·ªßa Academic Pressure")
    else:
        print("   ‚û§ K·∫æT LU·∫¨N: Study Satisfaction l√†m TƒÇNG th√™m t√°c ƒë·ªông ti√™u c·ª±c (unexpected)")
else:
    print("   ‚û§ K·∫æT LU·∫¨N: Kh√¥ng c√≥ b·∫±ng ch·ª©ng r√µ r√†ng v·ªÅ t√°c ƒë·ªông ƒëi·ªÅu ti·∫øt")
```

### Q5: C√≥ t·ªìn t·∫°i ng∆∞·ª°ng Academic Pressure l√†m nguy c∆° tr·∫ßm c·∫£m tƒÉng ƒë·ªôt bi·∫øn?

```python
# ============================================
# CELL 15: Q5 - Ng∆∞·ª°ng Academic Pressure
# ============================================

print("\n" + "-"*50)
print("Q5: C√≥ t·ªìn t·∫°i ng∆∞·ª°ng Academic Pressure l√†m nguy c∆° tr·∫ßm c·∫£m tƒÉng ƒë·ªôt bi·∫øn?")
print("-"*50)

# 1. T√≠nh depression rate v√† rate of change
pressure_levels = sorted(df_clean['Academic Pressure'].unique())
dep_rates = []
for level in pressure_levels:
    rate = df_clean[df_clean['Academic Pressure'] == level]['Depression'].mean() * 100
    dep_rates.append(rate)

# T√≠nh rate of change (gradient)
gradients = np.diff(dep_rates)

print("\nüìä Depression Rate v√† Rate of Change theo Academic Pressure:")
threshold_df = pd.DataFrame({
    'Pressure Level': pressure_levels,
    'Depression Rate (%)': dep_rates,
    'Change from Previous': ['-'] + [f'{g:.2f}%' for g in gradients]
})
display(threshold_df)

# 2. T√¨m ƒëi·ªÉm c√≥ gradient l·ªõn nh·∫•t (potential threshold)
max_gradient_idx = np.argmax(gradients)
threshold_level = pressure_levels[max_gradient_idx + 1]
max_gradient = gradients[max_gradient_idx]

print(f"\nüîç Ng∆∞·ª°ng ti·ªÅm nƒÉng ph√°t hi·ªán:")
print(f"   - M·ª©c tƒÉng ƒë·ªôt bi·∫øn l·ªõn nh·∫•t: t·ª´ level {pressure_levels[max_gradient_idx]} ‚Üí {threshold_level}")
print(f"   - M·ª©c tƒÉng: {max_gradient:.2f}% points")

# 3. Visualization
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Line plot with gradient visualization
ax1 = axes[0]
ax1.plot(pressure_levels, dep_rates, 'b-o', linewidth=2, markersize=10, label='Depression Rate')
ax1.fill_between(pressure_levels, dep_rates, alpha=0.3)

# Highlight threshold
ax1.axvline(threshold_level, color='red', linestyle='--', linewidth=2, 
            label=f'Potential Threshold (Level {threshold_level})')
ax1.set_xlabel('Academic Pressure Level')
ax1.set_ylabel('Depression Rate (%)')
ax1.set_title('Depression Rate theo Academic Pressure v·ªõi Ng∆∞·ª°ng', fontsize=11, fontweight='bold')
ax1.legend()
ax1.grid(True, alpha=0.3)

# Bar chart of gradients
ax2 = axes[1]
transition_labels = [f'{pressure_levels[i]}‚Üí{pressure_levels[i+1]}' for i in range(len(gradients))]
colors = ['red' if i == max_gradient_idx else 'steelblue' for i in range(len(gradients))]
bars = ax2.bar(transition_labels, gradients, color=colors, edgecolor='black')
ax2.axhline(0, color='black', linewidth=0.5)
ax2.set_xlabel('Transition')
ax2.set_ylabel('Change in Depression Rate (% points)')
ax2.set_title('Rate of Change gi·ªØa c√°c m·ª©c Academic Pressure', fontsize=11, fontweight='bold')
for bar in bars:
    height = bar.get_height()
    ax2.text(bar.get_x() + bar.get_width()/2., height + 0.5,
            f'{height:.1f}', ha='center', va='bottom', fontsize=9)

plt.tight_layout()
plt.show()

# 4. Statistical test - So s√°nh tr∆∞·ªõc v√† sau ng∆∞·ª°ng
print("\nüìà Ki·ªÉm ƒë·ªãnh th·ªëng k√™ t·∫°i ng∆∞·ª°ng:")
before_threshold = df_clean[df_clean['Academic Pressure'] < threshold_level]['Depression']
after_threshold = df_clean[df_clean['Academic Pressure'] >= threshold_level]['Depression']

# Two-proportion z-test
from statsmodels.stats.proportion import proportions_ztest

count = np.array([after_threshold.sum(), before_threshold.sum()])
nobs = np.array([len(after_threshold), len(before_threshold)])
z_stat, p_val = proportions_ztest(count, nobs, alternative='larger')

print(f"   T·ª∑ l·ªá tr·∫ßm c·∫£m tr∆∞·ªõc ng∆∞·ª°ng (<{threshold_level}): {before_threshold.mean()*100:.1f}%")
print(f"   T·ª∑ l·ªá tr·∫ßm c·∫£m t·∫°i v√† sau ng∆∞·ª°ng (‚â•{threshold_level}): {after_threshold.mean()*100:.1f}%")
print(f"   Z-statistic: {z_stat:.4f}")
print(f"   p-value: {p_val:.4e}")
print(f"\n   ‚û§ {'C√ì' if p_val < 0.05 else 'KH√îNG c√≥'} b·∫±ng ch·ª©ng th·ªëng k√™ v·ªÅ s·ª± tƒÉng ƒë·ªôt bi·∫øn t·∫°i ng∆∞·ª°ng {threshold_level}")
```

---

## 3.3. TH√ÄNH T√çCH H·ªåC T·∫¨P (COUNTER-INTUITIVE INSIGHT)

### Q6: CGPA c√≥ m·ªëi quan h·ªá tuy·∫øn t√≠nh v·ªõi tr·∫ßm c·∫£m kh√¥ng?

```python
# ============================================
# CELL 16: Q6 - CGPA v√† Depression linearity
# ============================================

print("\n" + "="*70)
print("3.3 TH√ÄNH T√çCH H·ªåC T·∫¨P (COUNTER-INTUITIVE INSIGHT)")
print("="*70)
print("\n" + "-"*50)
print("Q6: CGPA c√≥ m·ªëi quan h·ªá tuy·∫øn t√≠nh v·ªõi tr·∫ßm c·∫£m kh√¥ng?")
print("-"*50)

# 1. Correlation analysis
r_pearson, p_pearson = pearsonr(df_clean['CGPA'], df_clean['Depression'])
r_spearman, p_spearman = spearmanr(df_clean['CGPA'], df_clean['Depression'])

print(f"\nüìä Correlation Analysis:")
print(f"   Pearson r: {r_pearson:.4f} (p = {p_pearson:.4e})")
print(f"   Spearman œÅ: {r_spearman:.4f} (p = {p_spearman:.4e})")

# 2. Ph√¢n chia CGPA th√†nh c√°c nh√≥m
cgpa_bins = [0, 5, 6, 7, 8, 9, 11]
cgpa_labels = ['<5', '5-6', '6-7', '7-8', '8-9', '9+']
df_clean['CGPA_Group'] = pd.cut(df_clean['CGPA'], bins=cgpa_bins, labels=cgpa_labels)

cgpa_dep = df_clean.groupby('CGPA_Group').agg({
    'Depression': ['mean', 'count']
})
cgpa_dep.columns = ['Depression Rate', 'Count']
cgpa_dep['Depression %'] = (cgpa_dep['Depression Rate'] * 100).round(1)

print("\nüìä T·ª∑ l·ªá tr·∫ßm c·∫£m theo nh√≥m CGPA:")
display(cgpa_dep)

# 3. Visualization
fig, axes = plt.subplots(1, 3, figsize=(16, 5))

# Scatter plot v·ªõi regression line
ax1 = axes[0]
# Sample ƒë·ªÉ kh√¥ng qu√° ƒë√¥ng
sample_df = df_clean.sample(min(5000, len(df_clean)), random_state=42)
ax1.scatter(sample_df['CGPA'], sample_df['Depression'], alpha=0.1, s=10)
# Add trendline
z = np.polyfit(sample_df['CGPA'].dropna(), sample_df['Depression'].dropna(), 1)
p = np.poly1d(z)
x_line = np.linspace(sample_df['CGPA'].min(), sample_df['CGPA'].max(), 100)
ax1.plot(x_line, p(x_line), 'r-', linewidth=2, label=f'Linear fit (r={r_pearson:.3f})')
ax1.set_xlabel('CGPA')
ax1.set_ylabel('Depression (0/1)')
ax1.set_title('CGPA vs Depression (Linear)', fontsize=11, fontweight='bold')
ax1.legend()

# Bar chart - Depression rate by CGPA group
ax2 = axes[1]
bars = ax2.bar(cgpa_dep.index, cgpa_dep['Depression %'], color='teal', edgecolor='black')
ax2.set_xlabel('CGPA Group')
ax2.set_ylabel('Depression Rate (%)')
ax2.set_title('T·ª∑ l·ªá tr·∫ßm c·∫£m theo nh√≥m CGPA', fontsize=11, fontweight='bold')
for bar in bars:
    height = bar.get_height()
    ax2.text(bar.get_x() + bar.get_width()/2., height + 1,
            f'{height:.1f}%', ha='center', va='bottom', fontsize=9)

# Ki·ªÉm tra non-linearity b·∫±ng polynomial fit
ax3 = axes[2]
cgpa_midpoints = [4, 5.5, 6.5, 7.5, 8.5, 9.5]
dep_rates = cgpa_dep['Depression Rate'].values
ax3.scatter(cgpa_midpoints, dep_rates * 100, s=100, color='navy', zorder=5)
ax3.plot(cgpa_midpoints, dep_rates * 100, 'b-', linewidth=2, label='Actual')

# Polynomial fit degree 2
z2 = np.polyfit(cgpa_midpoints, dep_rates * 100, 2)
p2 = np.poly1d(z2)
x_smooth = np.linspace(min(cgpa_midpoints), max(cgpa_midpoints), 100)
ax3.plot(x_smooth, p2(x_smooth), 'r--', linewidth=2, label='Quadratic fit')

ax3.set_xlabel('CGPA')
ax3.set_ylabel('Depression Rate (%)')
ax3.set_title('Ki·ªÉm tra Non-linearity', fontsize=11, fontweight='bold')
ax3.legend()

plt.tight_layout()
plt.show()

# 4. Test for non-linearity
print("\nüìà Ki·ªÉm tra t√≠nh phi tuy·∫øn:")
# Polynomial regression comparison
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score

X_cgpa = df_clean[['CGPA']].dropna()
y_dep = df_clean.loc[X_cgpa.index, 'Depression']

# Linear
lr_linear = LinearRegression().fit(X_cgpa, y_dep)
r2_linear = r2_score(y_dep, lr_linear.predict(X_cgpa))

# Quadratic
poly2 = PolynomialFeatures(degree=2)
X_poly2 = poly2.fit_transform(X_cgpa)
lr_poly2 = LinearRegression().fit(X_poly2, y_dep)
r2_poly2 = r2_score(y_dep, lr_poly2.predict(X_poly2))

# Cubic
poly3 = PolynomialFeatures(degree=3)
X_poly3 = poly3.fit_transform(X_cgpa)
lr_poly3 = LinearRegression().fit(X_poly3, y_dep)
r2_poly3 = r2_score(y_dep, lr_poly3.predict(X_poly3))

print(f"   R¬≤ Linear:    {r2_linear:.6f}")
print(f"   R¬≤ Quadratic: {r2_poly2:.6f}")
print(f"   R¬≤ Cubic:     {r2_poly3:.6f}")

improvement = (r2_poly2 - r2_linear) / r2_linear * 100 if r2_linear != 0 else 0
print(f"\n   ‚û§ Quadratic improvement over linear: {improvement:.1f}%")
print(f"   ‚û§ M·ªëi quan h·ªá c√≥ v·∫ª {'PHI TUY·∫æN' if improvement > 10 else 'G·∫¶N NH∆Ø TUY·∫æN T√çNH'}")
```

### Q7: "High achievers but unhappy" c√≥ th·ª±c s·ª± t·ªìn t·∫°i?

```python
# ============================================
# CELL 17: Q7 - High achievers but unhappy
# ============================================

print("\n" + "-"*50)
print("Q7: 'High achievers but unhappy' c√≥ th·ª±c s·ª± t·ªìn t·∫°i?")
print("-"*50)

# 1. ƒê·ªãnh nghƒ©a High Achievers: CGPA >= 8.0
high_achievers = df_clean[df_clean['CGPA'] >= 8.0].copy()
others = df_clean[df_clean['CGPA'] < 8.0].copy()

print(f"\nüìä T·ªïng quan:")
print(f"   High Achievers (CGPA ‚â• 8.0): {len(high_achievers):,} ({len(high_achievers)/len(df_clean)*100:.1f}%)")
print(f"   Others (CGPA < 8.0): {len(others):,} ({len(others)/len(df_clean)*100:.1f}%)")

# 2. So s√°nh t·ª∑ l·ªá tr·∫ßm c·∫£m
dep_rate_high = high_achievers['Depression'].mean() * 100
dep_rate_others = others['Depression'].mean() * 100

print(f"\nüìä T·ª∑ l·ªá tr·∫ßm c·∫£m:")
print(f"   High Achievers: {dep_rate_high:.1f}%")
print(f"   Others: {dep_rate_others:.1f}%")

# 3. Ph√¢n t√≠ch s√¢u h∆°n: High achievers with low satisfaction
high_achievers['Low_Satisfaction'] = high_achievers['Study Satisfaction'] <= 2
low_sat_high_cgpa = high_achievers[high_achievers['Low_Satisfaction']]

print(f"\nüìä High Achievers v·ªõi Study Satisfaction th·∫•p (‚â§2):")
print(f"   S·ªë l∆∞·ª£ng: {len(low_sat_high_cgpa):,}")
print(f"   T·ª∑ l·ªá tr·∫ßm c·∫£m trong nh√≥m n√†y: {low_sat_high_cgpa['Depression'].mean()*100:.1f}%")

# 4. So s√°nh ƒëa chi·ªÅu
comparison_df = pd.DataFrame({
    'Metric': ['Depression Rate', 'Avg Academic Pressure', 'Avg Study Satisfaction', 
               'Avg Financial Stress', 'Suicidal Thoughts Rate'],
    'High Achievers (CGPA‚â•8)': [
        f"{high_achievers['Depression'].mean()*100:.1f}%",
        f"{high_achievers['Academic Pressure'].mean():.2f}",
        f"{high_achievers['Study Satisfaction'].mean():.2f}",
        f"{high_achievers['Financial Stress'].mean():.2f}",
        f"{high_achievers['Suicidal_Thoughts'].mean()*100:.1f}%"
    ],
    'Others (CGPA<8)': [
        f"{others['Depression'].mean()*100:.1f}%",
        f"{others['Academic Pressure'].mean():.2f}",
        f"{others['Study Satisfaction'].mean():.2f}",
        f"{others['Financial Stress'].mean():.2f}",
        f"{others['Suicidal_Thoughts'].mean()*100:.1f}%"
    ]
})
print("\nüìä So s√°nh chi ti·∫øt:")
display(comparison_df)

# 5. Visualization
fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# Plot 1: Depression rate by CGPA group and Satisfaction
ax1 = axes[0, 0]
# Create combined groups
df_clean['Achievement_Sat'] = df_clean.apply(
    lambda x: 'High CGPA + Low Satisfaction' if x['CGPA'] >= 8 and x['Study Satisfaction'] <= 2
    else 'High CGPA + High Satisfaction' if x['CGPA'] >= 8 and x['Study Satisfaction'] >= 4
    else 'Low CGPA + Low Satisfaction' if x['CGPA'] < 8 and x['Study Satisfaction'] <= 2
    else 'Low CGPA + High Satisfaction' if x['CGPA'] < 8 and x['Study Satisfaction'] >= 4
    else 'Medium',
    axis=1
)
group_order = ['Low CGPA + Low Satisfaction', 'Low CGPA + High Satisfaction',
               'High CGPA + Low Satisfaction', 'High CGPA + High Satisfaction']
group_dep = df_clean.groupby('Achievement_Sat')['Depression'].mean() * 100
group_dep = group_dep.reindex([g for g in group_order if g in group_dep.index])
bars = ax1.barh(group_dep.index, group_dep.values, color=['#e74c3c', '#2ecc71', '#f39c12', '#3498db'])
ax1.set_xlabel('Depression Rate (%)')
ax1.set_title('Depression Rate theo CGPA v√† Study Satisfaction', fontsize=11, fontweight='bold')
for bar in bars:
    width = bar.get_width()
    ax1.text(width + 1, bar.get_y() + bar.get_height()/2.,
            f'{width:.1f}%', ha='left', va='center', fontsize=9)

# Plot 2: Scatter - CGPA vs Study Satisfaction colored by Depression
ax2 = axes[0, 1]
colors = df_clean['Depression'].map({0: 'green', 1: 'red'})
scatter = ax2.scatter(df_clean['CGPA'], df_clean['Study Satisfaction'], 
                      c=colors, alpha=0.3, s=10)
ax2.set_xlabel('CGPA')
ax2.set_ylabel('Study Satisfaction')
ax2.set_title('CGPA vs Study Satisfaction (Green=No Depression, Red=Depression)', fontsize=10, fontweight='bold')
# Add quadrant lines
ax2.axhline(3, color='gray', linestyle='--', alpha=0.5)
ax2.axvline(8, color='gray', linestyle='--', alpha=0.5)
ax2.text(9, 1.5, 'High achievers\nbut unhappy', fontsize=9, ha='center', color='orange', fontweight='bold')

# Plot 3: High achievers breakdown
ax3 = axes[1, 0]
satisfaction_breakdown = high_achievers.groupby('Study Satisfaction')['Depression'].agg(['mean', 'count'])
satisfaction_breakdown['Depression Rate'] = satisfaction_breakdown['mean'] * 100
ax3.bar(satisfaction_breakdown.index, satisfaction_breakdown['Depression Rate'], 
        color=plt.cm.RdYlGn_r(np.linspace(0.2, 0.8, 5)), edgecolor='black')
ax3.set_xlabel('Study Satisfaction Level')
ax3.set_ylabel('Depression Rate (%)')
ax3.set_title('High Achievers: Depression Rate by Satisfaction', fontsize=11, fontweight='bold')
for i, (idx, row) in enumerate(satisfaction_breakdown.iterrows()):
    ax3.text(idx, row['Depression Rate'] + 1, f"{row['Depression Rate']:.1f}%\n(n={int(row['count'])})", 
             ha='center', fontsize=8)

# Plot 4: Academic Pressure comparison
ax4 = axes[1, 1]
pressure_data = [high_achievers[high_achievers['Depression']==0]['Academic Pressure'],
                 high_achievers[high_achievers['Depression']==1]['Academic Pressure']]
bp = ax4.boxplot(pressure_data, labels=['No Depression', 'Depression'], patch_artist=True)
bp['boxes'][0].set_facecolor('#2ecc71')
bp['boxes'][1].set_facecolor('#e74c3c')
ax4.set_ylabel('Academic Pressure')
ax4.set_title('High Achievers: Academic Pressure by Depression Status', fontsize=11, fontweight='bold')

plt.tight_layout()
plt.show()

# 6. Statistical test
print("\nüìà Ki·ªÉm ƒë·ªãnh th·ªëng k√™:")
# t-test: Academic Pressure gi·ªØa 2 nh√≥m high achievers
t_stat, t_p = stats.ttest_ind(
    high_achievers[high_achievers['Depression']==1]['Academic Pressure'],
    high_achievers[high_achievers['Depression']==0]['Academic Pressure']
)
print(f"   t-test (Academic Pressure - High Achievers with vs without Depression):")
print(f"   t = {t_stat:.4f}, p = {t_p:.4e}")
print(f"\n   ‚û§ {'C√ì' if t_p < 0.05 else 'KH√îNG c√≥'} s·ª± kh√°c bi·ªát c√≥ √Ω nghƒ©a th·ªëng k√™")
print(f"\nüìù K·∫æT LU·∫¨N:")
print(f"   - 'High achievers but unhappy' {'T·ªíN T·∫†I' if dep_rate_high > 30 else 'kh√¥ng ph·ªï bi·∫øn'}")
print(f"   - T·ª∑ l·ªá tr·∫ßm c·∫£m ·ªü High Achievers: {dep_rate_high:.1f}%")
print(f"   - ƒê·∫∑c bi·ªát, High Achievers v·ªõi Low Satisfaction c√≥ t·ª∑ l·ªá: {low_sat_high_cgpa['Depression'].mean()*100:.1f}%")
```

### Q8: Th·ªùi gian h·ªçc/l√†m vi·ªác c√≥ ·∫£nh h∆∞·ªüng ƒë·ªôc l·∫≠p v·ªõi CGPA kh√¥ng?

```python
# ============================================
# CELL 18: Q8 - Work/Study Hours ƒë·ªôc l·∫≠p v·ªõi CGPA
# ============================================

print("\n" + "-"*50)
print("Q8: Th·ªùi gian h·ªçc/l√†m vi·ªác c√≥ ·∫£nh h∆∞·ªüng ƒë·ªôc l·∫≠p v·ªõi CGPA kh√¥ng?")
print("-"*50)

# 1. Correlation gi·ªØa Work/Study Hours v√† CGPA
r_hours_cgpa, p_hours_cgpa = pearsonr(df_clean['Work/Study Hours'].dropna(), 
                                       df_clean.loc[df_clean['Work/Study Hours'].notna(), 'CGPA'])
print(f"\nüìä Correlation gi·ªØa Work/Study Hours v√† CGPA:")
print(f"   r = {r_hours_cgpa:.4f} (p = {p_hours_cgpa:.4e})")

# 2. Partial correlation - ki·ªÉm so√°t CGPA
# Manual calculation of partial correlation
from scipy.stats import pearsonr

def partial_corr(x, y, control):
    """Calculate partial correlation between x and y, controlling for control"""
    r_xy = pearsonr(x, y)[0]
    r_xc = pearsonr(x, control)[0]
    r_yc = pearsonr(y, control)[0]
    
    partial_r = (r_xy - r_xc * r_yc) / np.sqrt((1 - r_xc**2) * (1 - r_yc**2))
    return partial_r

# Prepare data
model_data = df_clean[['Work/Study Hours', 'CGPA', 'Depression']].dropna()
partial_r = partial_corr(model_data['Work/Study Hours'], 
                         model_data['Depression'], 
                         model_data['CGPA'])

print(f"\nüìä Partial Correlation (Work/Study Hours ~ Depression | CGPA):")
print(f"   Partial r = {partial_r:.4f}")

# 3. Multiple Regression
print("\nüìà Multiple Regression Analysis:")
X_multi = model_data[['Work/Study Hours', 'CGPA']]
X_multi = sm.add_constant(X_multi)
y_multi = model_data['Depression']

model_multi = sm.OLS(y_multi, X_multi).fit()
print(model_multi.summary2().tables[1])

# 4. Ph√¢n t√≠ch theo nh√≥m gi·ªù h·ªçc
hours_bins = [0, 3, 6, 9, 15]
hours_labels = ['0-3h', '3-6h', '6-9h', '9h+']
df_clean['Hours_Group'] = pd.cut(df_clean['Work/Study Hours'], bins=hours_bins, labels=hours_labels)

# Depression rate by hours group, controlling for CGPA
print("\nüìä T·ª∑ l·ªá tr·∫ßm c·∫£m theo Work/Study Hours (ph√¢n theo CGPA):")
hours_cgpa_dep = df_clean.groupby(['Hours_Group', 'CGPA_Group'])['Depression'].mean() * 100
hours_cgpa_pivot = hours_cgpa_dep.unstack()
display(hours_cgpa_pivot.round(1))

# 5. Visualization
fig, axes = plt.subplots(1, 3, figsize=(16, 5))

# Scatter plot: Hours vs Depression with CGPA coloring
ax1 = axes[0]
sample = df_clean.sample(min(3000, len(df_clean)), random_state=42)
scatter = ax1.scatter(sample['Work/Study Hours'], sample['Depression'] + np.random.normal(0, 0.05, len(sample)),
                      c=sample['CGPA'], cmap='viridis', alpha=0.5, s=10)
plt.colorbar(scatter, ax=ax1, label='CGPA')
ax1.set_xlabel('Work/Study Hours')
ax1.set_ylabel('Depression (jittered)')
ax1.set_title('Hours vs Depression, colored by CGPA', fontsize=11, fontweight='bold')

# Bar chart: Depression by Hours Group
ax2 = axes[1]
hours_dep = df_clean.groupby('Hours_Group')['Depression'].mean() * 100
bars = ax2.bar(hours_dep.index, hours_dep.values, color='steelblue', edgecolor='black')
ax2.set_xlabel('Work/Study Hours')
ax2.set_ylabel('Depression Rate (%)')
ax2.set_title('Depression Rate by Work/Study Hours', fontsize=11, fontweight='bold')
for bar in bars:
    height = bar.get_height()
    ax2.text(bar.get_x() + bar.get_width()/2., height + 1,
            f'{height:.1f}%', ha='center', fontsize=9)

# Heatmap: Hours x CGPA
ax3 = axes[2]
sns.heatmap(hours_cgpa_pivot.T, annot=True, fmt='.1f', cmap='YlOrRd', ax=ax3)
ax3.set_title('Depression Rate: Hours √ó CGPA', fontsize=11, fontweight='bold')
ax3.set_xlabel('Work/Study Hours')
ax3.set_ylabel('CGPA Group')

plt.tight_layout()
plt.show()

# 6. VIF ƒë·ªÉ ki·ªÉm tra multicollinearity
print("\nüìä Variance Inflation Factor (VIF):")
from statsmodels.stats.outliers_influence import variance_inflation_factor

X_vif = model_data[['Work/Study Hours', 'CGPA']]
vif_data = pd.DataFrame()
vif_data['Feature'] = X_vif.columns
vif_data['VIF'] = [variance_inflation_factor(X_vif.values, i) for i in range(X_vif.shape[1])]
display(vif_data)
print("   VIF < 5: Kh√¥ng c√≥ multicollinearity nghi√™m tr·ªçng")
print("   VIF > 10: Multicollinearity nghi√™m tr·ªçng")

print(f"\nüìù K·∫æT LU·∫¨N:")
print(f"   - Partial correlation (controlling for CGPA): {partial_r:.4f}")
print(f"   - Work/Study Hours {'C√ì' if abs(partial_r) > 0.1 else 'KH√îNG c√≥'} ·∫£nh h∆∞·ªüng ƒë·ªôc l·∫≠p v·ªõi CGPA")
```

---

## 3.4. L·ªêI S·ªêNG & S·ª®C KH·ªéE TINH TH·∫¶N

### Q9: Gi·∫•c ng·ªß ·∫£nh h∆∞·ªüng ƒë·∫øn tr·∫ßm c·∫£m m·∫°nh ƒë·∫øn m·ª©c n√†o?

```python
# ============================================
# CELL 19: Q9 - Sleep Duration v√† Depression
# ============================================

print("\n" + "="*70)
print("3.4 L·ªêI S·ªêNG & S·ª®C KH·ªéE TINH TH·∫¶N")
print("="*70)
print("\n" + "-"*50)
print("Q9: Gi·∫•c ng·ªß ·∫£nh h∆∞·ªüng ƒë·∫øn tr·∫ßm c·∫£m m·∫°nh ƒë·∫øn m·ª©c n√†o?")
print("-"*50)

# 1. T·ª∑ l·ªá tr·∫ßm c·∫£m theo Sleep Duration
sleep_dep = df_clean.groupby('Sleep Duration').agg({
    'Depression': ['mean', 'count']
})
sleep_dep.columns = ['Depression Rate', 'Count']
sleep_dep['Depression %'] = (sleep_dep['Depression Rate'] * 100).round(1)

# S·∫Øp x·∫øp theo th·ª© t·ª± h·ª£p l√Ω
sleep_order = ['Less than 5 hours', '5-6 hours', '7-8 hours', 'More than 8 hours']
sleep_dep = sleep_dep.reindex(sleep_order)

print("\nüìä T·ª∑ l·ªá tr·∫ßm c·∫£m theo Sleep Duration:")
display(sleep_dep)

# 2. Chi-square test
contingency_sleep = pd.crosstab(df_clean['Sleep Duration'], df_clean['Depression'])
chi2_sleep, p_sleep, dof_sleep, expected_sleep = chi2_contingency(contingency_sleep)
print(f"\nüìà Chi-square test:")
print(f"   œá¬≤ = {chi2_sleep:.4f}, p = {p_sleep:.4e}")

# 3. Odds Ratio - So s√°nh <5 hours vs 7-8 hours
sleep_lt5 = df_clean[df_clean['Sleep Duration'] == 'Less than 5 hours']
sleep_7_8 = df_clean[df_clean['Sleep Duration'] == '7-8 hours']

dep_lt5 = sleep_lt5['Depression'].sum()
nodep_lt5 = len(sleep_lt5) - dep_lt5
dep_7_8 = sleep_7_8['Depression'].sum()
nodep_7_8 = len(sleep_7_8) - dep_7_8

odds_lt5 = dep_lt5 / nodep_lt5
odds_7_8 = dep_7_8 / nodep_7_8
or_sleep = odds_lt5 / odds_7_8

print(f"\nüìä Odds Ratio (<5 hours vs 7-8 hours):")
print(f"   OR = {or_sleep:.2f}")
print(f"   ‚û§ Ng·ªß <5 gi·ªù c√≥ nguy c∆° tr·∫ßm c·∫£m cao g·∫•p {or_sleep:.1f} l·∫ßn so v·ªõi ng·ªß 7-8 gi·ªù")

# 4. Visualization
fig, axes = plt.subplots(1, 3, figsize=(16, 5))

# Bar chart
ax1 = axes[0]
colors_sleep = ['#e74c3c', '#f39c12', '#2ecc71', '#3498db']
bars = ax1.bar(range(len(sleep_dep)), sleep_dep['Depression %'], color=colors_sleep, edgecolor='black')
ax1.set_xticks(range(len(sleep_dep)))
ax1.set_xticklabels(sleep_dep.index, rotation=15, ha='right')
ax1.set_ylabel('Depression Rate (%)')
ax1.set_title('T·ª∑ l·ªá tr·∫ßm c·∫£m theo Sleep Duration', fontsize=11, fontweight='bold')
for bar in bars:
    height = bar.get_height()
    ax1.text(bar.get_x() + bar.get_width()/2., height + 1,
            f'{height:.1f}%', ha='center', fontsize=9)

# Grouped bar chart - Sleep x Depression status
ax2 = axes[1]
sleep_counts = pd.crosstab(df_clean['Sleep Duration'], df_clean['Depression'])
sleep_counts = sleep_counts.reindex(sleep_order)
sleep_counts.plot(kind='bar', ax=ax2, color=['#2ecc71', '#e74c3c'], edgecolor='black')
ax2.set_xlabel('Sleep Duration')
ax2.set_ylabel('Count')
ax2.set_title('S·ªë l∆∞·ª£ng theo Sleep Duration v√† Depression', fontsize=11, fontweight='bold')
ax2.legend(['No Depression', 'Depression'])
ax2.tick_params(axis='x', rotation=15)

# Relative Risk visualization
ax3 = axes[2]
reference = sleep_dep.loc['7-8 hours', 'Depression Rate']
relative_risk = sleep_dep['Depression Rate'] / reference
ax3.barh(relative_risk.index, relative_risk.values, color=colors_sleep)
ax3.axvline(1, color='black', linestyle='--', linewidth=2, label='Reference (7-8 hours)')
ax3.set_xlabel('Relative Risk (vs 7-8 hours)')
ax3.set_title('Relative Risk c·ªßa Depression theo Sleep Duration', fontsize=11, fontweight='bold')
for i, (idx, val) in enumerate(relative_risk.items()):
    ax3.text(val + 0.02, i, f'{val:.2f}', va='center', fontsize=9)
ax3.legend()

plt.tight_layout()
plt.show()

print(f"\nüìù K·∫æT LU·∫¨N:")
print(f"   - Gi·∫•c ng·ªß c√≥ ·∫£nh h∆∞·ªüng M·∫†NH ƒë·∫øn tr·∫ßm c·∫£m (p < 0.001)")
print(f"   - Ng·ªß d∆∞·ªõi 5 gi·ªù: {sleep_dep.loc['Less than 5 hours', 'Depression %']:.1f}% tr·∫ßm c·∫£m")
print(f"   - Ng·ªß 7-8 gi·ªù: {sleep_dep.loc['7-8 hours', 'Depression %']:.1f}% tr·∫ßm c·∫£m")
print(f"   - OR = {or_sleep:.2f} (thi·∫øu ng·ªß tƒÉng g·∫•p {or_sleep:.1f}x nguy c∆°)")
```

### Q10: Sleep Duration c√≤n quan tr·ªçng khi ƒë√£ ki·ªÉm so√°t Academic Pressure kh√¥ng?

```python
# ============================================
# CELL 20: Q10 - Sleep Duration controlling for Academic Pressure
# ============================================

print("\n" + "-"*50)
print("Q10: Sleep Duration c√≤n quan tr·ªçng khi ƒë√£ ki·ªÉm so√°t Academic Pressure kh√¥ng?")
print("-"*50)

# 1. Logistic Regression v·ªõi c·∫£ 2 bi·∫øn
model_data_sleep = df_clean[['Sleep_Hours_Encoded', 'Academic Pressure', 'Depression']].dropna()

X_sleep = model_data_sleep[['Sleep_Hours_Encoded', 'Academic Pressure']]
X_sleep = sm.add_constant(X_sleep)
y_sleep = model_data_sleep['Depression']

model_sleep = sm.Logit(y_sleep, X_sleep).fit(disp=0)
print("\nüìà Logistic Regression (Sleep + Academic Pressure):")
print(model_sleep.summary2().tables[1])

# 2. Stratified analysis - Depression rate by Sleep, stratified by Academic Pressure
df_clean['AP_Group'] = pd.cut(df_clean['Academic Pressure'], bins=[0, 2, 4, 6], labels=['Low (1-2)', 'Medium (3-4)', 'High (5)'])

stratified = df_clean.groupby(['Sleep Duration', 'AP_Group'])['Depression'].mean() * 100
stratified_pivot = stratified.unstack()
stratified_pivot = stratified_pivot.reindex(sleep_order)

print("\nüìä T·ª∑ l·ªá tr·∫ßm c·∫£m theo Sleep Duration, stratified by Academic Pressure:")
display(stratified_pivot.round(1))

# 3. Visualization
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Stratified bar chart
ax1 = axes[0]
stratified_pivot.plot(kind='bar', ax=ax1, color=['#2ecc71', '#f39c12', '#e74c3c'], edgecolor='black')
ax1.set_xlabel('Sleep Duration')
ax1.set_ylabel('Depression Rate (%)')
ax1.set_title('Depression Rate: Sleep √ó Academic Pressure', fontsize=11, fontweight='bold')
ax1.legend(title='Academic Pressure')
ax1.tick_params(axis='x', rotation=15)

# Coefficient comparison
ax2 = axes[1]
coef_names = ['Sleep_Hours_Encoded', 'Academic Pressure']
coef_vals = [model_sleep.params['Sleep_Hours_Encoded'], model_sleep.params['Academic Pressure']]
colors = ['green' if v < 0 else 'red' for v in coef_vals]
bars = ax2.barh(coef_names, coef_vals, color=colors, edgecolor='black')
ax2.axvline(0, color='black', linewidth=0.5)
ax2.set_xlabel('Coefficient (Log Odds)')
ax2.set_title('Logistic Regression Coefficients', fontsize=11, fontweight='bold')
for bar in bars:
    width = bar.get_width()
    ax2.text(width + 0.01 if width > 0 else width - 0.05, bar.get_y() + bar.get_height()/2.,
            f'{width:.3f}', va='center', fontsize=10)

plt.tight_layout()
plt.show()

# 4. Calculate odds ratios
print("\nüìä Odds Ratios t·ª´ Logistic Regression:")
odds_ratios = np.exp(model_sleep.params)
print(f"   Sleep_Hours_Encoded: OR = {odds_ratios['Sleep_Hours_Encoded']:.3f}")
print(f"   Academic Pressure: OR = {odds_ratios['Academic Pressure']:.3f}")

print(f"\nüìù GI·∫¢I TH√çCH:")
print(f"   - Sleep OR < 1: Ng·ªß nhi·ªÅu h∆°n ‚Üí gi·∫£m nguy c∆° tr·∫ßm c·∫£m")
print(f"   - C·ª© tƒÉng 1 m·ª©c sleep category ‚Üí OR gi·∫£m {(1-odds_ratios['Sleep_Hours_Encoded'])*100:.1f}%")
print(f"\nüìù K·∫æT LU·∫¨N:")
if model_sleep.pvalues['Sleep_Hours_Encoded'] < 0.05:
    print(f"   ‚úÖ Sleep Duration V·∫™N C√íN QUAN TR·ªåNG sau khi ki·ªÉm so√°t Academic Pressure")
    print(f"      (p = {model_sleep.pvalues['Sleep_Hours_Encoded']:.4e})")
else:
    print(f"   ‚ùå Sleep Duration KH√îNG C√íN QUAN TR·ªåNG sau khi ki·ªÉm so√°t Academic Pressure")
```

### Q11: Dietary Habits c√≥ t√°c ƒë·ªông tr·ª±c ti·∫øp hay gi√°n ti·∫øp qua gi·∫•c ng·ªß?

```python
# ============================================
# CELL 21: Q11 - Dietary Habits pathway analysis
# ============================================

print("\n" + "-"*50)
print("Q11: Dietary Habits c√≥ t√°c ƒë·ªông tr·ª±c ti·∫øp hay gi√°n ti·∫øp qua gi·∫•c ng·ªß?")
print("-"*50)

# 1. Direct relationship: Diet ‚Üí Depression
diet_dep = df_clean.groupby('Dietary Habits')['Depression'].mean() * 100
diet_order = ['Unhealthy', 'Moderate', 'Healthy']
diet_dep = diet_dep.reindex(diet_order)

print("\nüìä T·ª∑ l·ªá tr·∫ßm c·∫£m theo Dietary Habits:")
for diet, rate in diet_dep.items():
    print(f"   {diet}: {rate:.1f}%")

# 2. Relationship: Diet ‚Üí Sleep
diet_sleep = pd.crosstab(df_clean['Dietary Habits'], df_clean['Sleep Duration'], normalize='index') * 100
diet_sleep = diet_sleep.reindex(diet_order)
print("\nüìä Ph√¢n ph·ªëi Sleep Duration theo Dietary Habits (%):")
display(diet_sleep.round(1))

# 3. Mediation Analysis (simplified - Baron & Kenny approach)
print("\nüìà Mediation Analysis (Baron & Kenny Steps):")

# Step 1: X ‚Üí Y (Diet ‚Üí Depression)
model_data_med = df_clean[['Diet_Encoded', 'Sleep_Hours_Encoded', 'Depression']].dropna()

X1 = sm.add_constant(model_data_med['Diet_Encoded'])
y = model_data_med['Depression']
model_step1 = sm.Logit(y, X1).fit(disp=0)
c = model_step1.params['Diet_Encoded']
p_c = model_step1.pvalues['Diet_Encoded']
print(f"\n   Step 1 (Diet ‚Üí Depression): c = {c:.4f}, p = {p_c:.4e}")

# Step 2: X ‚Üí M (Diet ‚Üí Sleep)
X2 = sm.add_constant(model_data_med['Diet_Encoded'])
m = model_data_med['Sleep_Hours_Encoded']
model_step2 = sm.OLS(m, X2).fit()
a = model_step2.params['Diet_Encoded']
p_a = model_step2.pvalues['Diet_Encoded']
print(f"   Step 2 (Diet ‚Üí Sleep): a = {a:.4f}, p = {p_a:.4e}")

# Step 3: X, M ‚Üí Y (Diet, Sleep ‚Üí Depression)
X3 = sm.add_constant(model_data_med[['Diet_Encoded', 'Sleep_Hours_Encoded']])
model_step3 = sm.Logit(y, X3).fit(disp=0)
c_prime = model_step3.params['Diet_Encoded']
b = model_step3.params['Sleep_Hours_Encoded']
p_c_prime = model_step3.pvalues['Diet_Encoded']
p_b = model_step3.pvalues['Sleep_Hours_Encoded']
print(f"   Step 3 (Diet + Sleep ‚Üí Depression):")
print(f"      c' (Diet|Sleep) = {c_prime:.4f}, p = {p_c_prime:.4e}")
print(f"      b (Sleep|Diet) = {b:.4f}, p = {p_b:.4e}")

# Calculate indirect effect
indirect = a * b
total = c
direct = c_prime
proportion_mediated = (indirect / total * 100) if total != 0 else 0

print(f"\nüìä Ph√¢n t√≠ch Mediation:")
print(f"   Total effect (c): {total:.4f}")
print(f"   Direct effect (c'): {direct:.4f}")
print(f"   Indirect effect (a*b): {indirect:.4f}")
print(f"   Proportion mediated: {abs(proportion_mediated):.1f}%")

# 4. Visualization
fig, axes = plt.subplots(1, 3, figsize=(16, 5))

# Diet ‚Üí Depression
ax1 = axes[0]
colors_diet = ['#e74c3c', '#f39c12', '#2ecc71']
bars = ax1.bar(diet_dep.index, diet_dep.values, color=colors_diet, edgecolor='black')
ax1.set_xlabel('Dietary Habits')
ax1.set_ylabel('Depression Rate (%)')
ax1.set_title('Direct: Dietary Habits ‚Üí Depression', fontsize=11, fontweight='bold')
for bar in bars:
    height = bar.get_height()
    ax1.text(bar.get_x() + bar.get_width()/2., height + 1, f'{height:.1f}%', ha='center')

# Diet ‚Üí Sleep distribution
ax2 = axes[1]
diet_sleep[['Less than 5 hours', '5-6 hours', '7-8 hours', 'More than 8 hours']].plot(
    kind='bar', stacked=True, ax=ax2, colormap='RdYlGn')
ax2.set_xlabel('Dietary Habits')
ax2.set_ylabel('Percentage (%)')
ax2.set_title('Dietary Habits ‚Üí Sleep Duration Distribution', fontsize=11, fontweight='bold')
ax2.legend(title='Sleep Duration', bbox_to_anchor=(1.02, 1))
ax2.tick_params(axis='x', rotation=0)

# Mediation diagram (simplified visualization)
ax3 = axes[2]
ax3.axis('off')
# Draw boxes and arrows
ax3.add_patch(plt.Rectangle((0.1, 0.6), 0.25, 0.15, fill=True, facecolor='lightblue', edgecolor='black'))
ax3.add_patch(plt.Rectangle((0.6, 0.6), 0.25, 0.15, fill=True, facecolor='lightgreen', edgecolor='black'))
ax3.add_patch(plt.Rectangle((0.35, 0.2), 0.25, 0.15, fill=True, facecolor='lightyellow', edgecolor='black'))

ax3.text(0.225, 0.675, 'Dietary\nHabits', ha='center', va='center', fontsize=10, fontweight='bold')
ax3.text(0.725, 0.675, 'Depression', ha='center', va='center', fontsize=10, fontweight='bold')
ax3.text(0.475, 0.275, 'Sleep\nDuration', ha='center', va='center', fontsize=10, fontweight='bold')

# Arrows
ax3.annotate('', xy=(0.6, 0.675), xytext=(0.35, 0.675),
            arrowprops=dict(arrowstyle='->', color='blue', lw=2))
ax3.annotate('', xy=(0.35, 0.35), xytext=(0.225, 0.6),
            arrowprops=dict(arrowstyle='->', color='green', lw=2))
ax3.annotate('', xy=(0.725, 0.6), xytext=(0.6, 0.35),
            arrowprops=dict(arrowstyle='->', color='green', lw=2))

# Labels
ax3.text(0.475, 0.72, f"c' = {direct:.3f}", fontsize=9, ha='center')
ax3.text(0.22, 0.45, f"a = {a:.3f}", fontsize=9, ha='center')
ax3.text(0.72, 0.45, f"b = {b:.3f}", fontsize=9, ha='center')
ax3.set_title('Mediation Model', fontsize=11, fontweight='bold', y=0.95)

plt.tight_layout()
plt.show()

print(f"\nüìù K·∫æT LU·∫¨N:")
if abs(proportion_mediated) > 30:
    print(f"   - Sleep Duration l√† mediator QUAN TR·ªåNG ({abs(proportion_mediated):.1f}% effect ƒë∆∞·ª£c mediate)")
    print(f"   - Dietary Habits c√≥ c·∫£ t√°c ƒë·ªông tr·ª±c ti·∫øp V√Ä gi√°n ti·∫øp qua Sleep")
else:
    print(f"   - Sleep Duration l√† mediator Y·∫æU ({abs(proportion_mediated):.1f}% effect ƒë∆∞·ª£c mediate)")
    print(f"   - Dietary Habits ch·ªß y·∫øu c√≥ t√°c ƒë·ªông TR·ª∞C TI·∫æP ƒë·∫øn Depression")
```

---

## 3.5. STRESS T√ÇM L√ù & Y·∫æU T·ªê N·ªÄN

### Q12: Financial Stress hay Academic Pressure t√°c ƒë·ªông m·∫°nh h∆°n?

```python
# ============================================
# CELL 22: Q12 - Financial Stress vs Academic Pressure
# ============================================

print("\n" + "="*70)
print("3.5 STRESS T√ÇM L√ù & Y·∫æU T·ªê N·ªÄN")
print("="*70)
print("\n" + "-"*50)
print("Q12: Financial Stress hay Academic Pressure t√°c ƒë·ªông m·∫°nh h∆°n?")
print("-"*50)

# 1. Bivariate correlations
r_ap, p_ap = pointbiserialr(df_clean['Depression'], df_clean['Academic Pressure'])
r_fs, p_fs = pointbiserialr(df_clean['Depression'], df_clean['Financial Stress'])

print(f"\nüìä Point-biserial Correlations:")
print(f"   Academic Pressure ‚Üí Depression: r = {r_ap:.4f} (p = {p_ap:.4e})")
print(f"   Financial Stress ‚Üí Depression: r = {r_fs:.4f} (p = {p_fs:.4e})")

# 2. Logistic Regression - standardized coefficients
from sklearn.preprocessing import StandardScaler

model_data_stress = df_clean[['Academic Pressure', 'Financial Stress', 'Depression']].dropna()

# Standardize
scaler = StandardScaler()
X_std = scaler.fit_transform(model_data_stress[['Academic Pressure', 'Financial Stress']])
X_std_df = pd.DataFrame(X_std, columns=['Academic Pressure (std)', 'Financial Stress (std)'])
X_std_df = sm.add_constant(X_std_df)
y_stress = model_data_stress['Depression'].values

model_std = sm.Logit(y_stress, X_std_df).fit(disp=0)
print("\nüìà Logistic Regression v·ªõi Standardized Coefficients:")
print(model_std.summary2().tables[1])

# 3. Odds Ratios
print("\nüìä Odds Ratios (per 1 SD increase):")
for var in ['Academic Pressure (std)', 'Financial Stress (std)']:
    or_val = np.exp(model_std.params[var])
    ci_low = np.exp(model_std.conf_int().loc[var, 0])
    ci_high = np.exp(model_std.conf_int().loc[var, 1])
    print(f"   {var}: OR = {or_val:.3f} (95% CI: {ci_low:.3f} - {ci_high:.3f})")

# 4. Depression rate by levels
fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# AP effect
ax1 = axes[0, 0]
ap_dep = df_clean.groupby('Academic Pressure')['Depression'].mean() * 100
ax1.bar(ap_dep.index, ap_dep.values, color='coral', edgecolor='black')
ax1.set_xlabel('Academic Pressure Level')
ax1.set_ylabel('Depression Rate (%)')
ax1.set_title('Depression Rate by Academic Pressure', fontsize=11, fontweight='bold')
for i, v in enumerate(ap_dep.values):
    ax1.text(ap_dep.index[i], v + 1, f'{v:.1f}%', ha='center', fontsize=9)

# FS effect
ax2 = axes[0, 1]
fs_dep = df_clean.groupby('Financial Stress')['Depression'].mean() * 100
ax2.bar(fs_dep.index, fs_dep.values, color='steelblue', edgecolor='black')
ax2.set_xlabel('Financial Stress Level')
ax2.set_ylabel('Depression Rate (%)')
ax2.set_title('Depression Rate by Financial Stress', fontsize=11, fontweight='bold')
for i, v in enumerate(fs_dep.values):
    ax2.text(fs_dep.index[i], v + 1, f'{v:.1f}%', ha='center', fontsize=9)

# Coefficient comparison
ax3 = axes[1, 0]
coefs = pd.Series({
    'Academic\nPressure': model_std.params['Academic Pressure (std)'],
    'Financial\nStress': model_std.params['Financial Stress (std)']
})
colors = ['coral', 'steelblue']
bars = ax3.bar(coefs.index, coefs.values, color=colors, edgecolor='black')
ax3.set_ylabel('Standardized Coefficient')
ax3.set_title('So s√°nh Standardized Coefficients', fontsize=11, fontweight='bold')
for bar in bars:
    height = bar.get_height()
    ax3.text(bar.get_x() + bar.get_width()/2., height + 0.02,
            f'{height:.3f}', ha='center', fontsize=10)

# Interaction heatmap
ax4 = axes[1, 1]
interaction_fs_ap = df_clean.groupby(['Academic Pressure', 'Financial Stress'])['Depression'].mean() * 100
interaction_pivot = interaction_fs_ap.unstack()
sns.heatmap(interaction_pivot, annot=True, fmt='.1f', cmap='YlOrRd', ax=ax4)
ax4.set_title('Depression Rate: AP √ó FS', fontsize=11, fontweight='bold')
ax4.set_xlabel('Financial Stress')
ax4.set_ylabel('Academic Pressure')

plt.tight_layout()
plt.show()

# 5. K·∫øt lu·∫≠n
winner = 'Academic Pressure' if abs(model_std.params['Academic Pressure (std)']) > abs(model_std.params['Financial Stress (std)']) else 'Financial Stress'
ratio = abs(model_std.params['Academic Pressure (std)']) / abs(model_std.params['Financial Stress (std)'])

print(f"\nüìù K·∫æT LU·∫¨N:")
print(f"   - {winner} c√≥ t√°c ƒë·ªông M·∫†NH H∆†N ƒë·∫øn Depression")
print(f"   - T·ª∑ l·ªá coefficient: AP/FS = {ratio:.2f}")
print(f"   - C·∫£ hai ƒë·ªÅu c√≥ √Ω nghƒ©a th·ªëng k√™ (p < 0.05)")
```

### Q13: Suicidal Thoughts c√≥ th·ªÉ xem l√† t√≠n hi·ªáu c·∫£nh b√°o s·ªõm kh√¥ng?

```python
# ============================================
# CELL 23: Q13 - Suicidal Thoughts as early warning
# ============================================

print("\n" + "-"*50)
print("Q13: Suicidal Thoughts c√≥ th·ªÉ xem l√† t√≠n hi·ªáu c·∫£nh b√°o s·ªõm kh√¥ng?")
print("-"*50)

# 1. Cross-tabulation
suicidal_dep = pd.crosstab(df_clean['Have you ever had suicidal thoughts ?'], 
                           df_clean['Depression'], margins=True)
suicidal_dep_pct = pd.crosstab(df_clean['Have you ever had suicidal thoughts ?'], 
                               df_clean['Depression'], normalize='index') * 100

print("\nüìä Cross-tabulation (Count):")
display(suicidal_dep)
print("\nüìä Cross-tabulation (%):")
display(suicidal_dep_pct.round(1))

# 2. Key metrics
# Sensitivity (True Positive Rate): P(Suicidal=Yes | Depression=Yes)
# Specificity (True Negative Rate): P(Suicidal=No | Depression=No)
# PPV (Positive Predictive Value): P(Depression=Yes | Suicidal=Yes)
# NPV (Negative Predictive Value): P(Depression=No | Suicidal=No)

dep_data = df_clean.dropna(subset=['Suicidal_Thoughts', 'Depression'])
TP = len(dep_data[(dep_data['Suicidal_Thoughts']==1) & (dep_data['Depression']==1)])
TN = len(dep_data[(dep_data['Suicidal_Thoughts']==0) & (dep_data['Depression']==0)])
FP = len(dep_data[(dep_data['Suicidal_Thoughts']==1) & (dep_data['Depression']==0)])
FN = len(dep_data[(dep_data['Suicidal_Thoughts']==0) & (dep_data['Depression']==1)])

sensitivity = TP / (TP + FN) if (TP + FN) > 0 else 0
specificity = TN / (TN + FP) if (TN + FP) > 0 else 0
ppv = TP / (TP + FP) if (TP + FP) > 0 else 0
npv = TN / (TN + FN) if (TN + FN) > 0 else 0

print("\nüìä Diagnostic Metrics (Suicidal Thoughts as predictor):")
print(f"   Sensitivity (Recall): {sensitivity*100:.1f}%")
print(f"   Specificity: {specificity*100:.1f}%")
print(f"   PPV (Precision): {ppv*100:.1f}%")
print(f"   NPV: {npv*100:.1f}%")

# 3. Odds Ratio
odds_suicidal = TP / FP if FP > 0 else np.inf
odds_no_suicidal = FN / TN if TN > 0 else np.inf
or_suicidal = odds_suicidal / odds_no_suicidal if odds_no_suicidal > 0 else np.inf

print(f"\nüìä Odds Ratio:")
print(f"   OR = {or_suicidal:.2f}")
print(f"   ‚û§ Ng∆∞·ªùi c√≥ Suicidal Thoughts c√≥ nguy c∆° Depression cao g·∫•p {or_suicidal:.1f} l·∫ßn")

# 4. Visualization
fig, axes = plt.subplots(1, 3, figsize=(16, 5))

# Stacked bar chart
ax1 = axes[0]
suicidal_dep_pct.drop('All', errors='ignore').plot(kind='bar', stacked=True, 
                                                    ax=ax1, color=['#2ecc71', '#e74c3c'],
                                                    edgecolor='black')
ax1.set_xlabel('Suicidal Thoughts')
ax1.set_ylabel('Percentage (%)')
ax1.set_title('Depression Rate by Suicidal Thoughts', fontsize=11, fontweight='bold')
ax1.legend(['No Depression', 'Depression'])
ax1.tick_params(axis='x', rotation=0)

# Confusion Matrix style
ax2 = axes[1]
cm = np.array([[TN, FP], [FN, TP]])
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax2,
            xticklabels=['No Depression', 'Depression'],
            yticklabels=['No Suicidal', 'Suicidal'])
ax2.set_xlabel('Actual Depression Status')
ax2.set_ylabel('Suicidal Thoughts')
ax2.set_title('Confusion Matrix', fontsize=11, fontweight='bold')

# Metrics visualization
ax3 = axes[2]
metrics = ['Sensitivity', 'Specificity', 'PPV', 'NPV']
values = [sensitivity, specificity, ppv, npv]
colors = ['#3498db', '#2ecc71', '#f39c12', '#9b59b6']
bars = ax3.bar(metrics, [v*100 for v in values], color=colors, edgecolor='black')
ax3.set_ylabel('Percentage (%)')
ax3.set_title('Diagnostic Performance Metrics', fontsize=11, fontweight='bold')
ax3.axhline(50, color='gray', linestyle='--', alpha=0.5)
for bar in bars:
    height = bar.get_height()
    ax3.text(bar.get_x() + bar.get_width()/2., height + 1,
            f'{height:.1f}%', ha='center', fontsize=10)

plt.tight_layout()
plt.show()

# 5. Chi-square test
chi2_suicidal, p_suicidal, _, _ = chi2_contingency(suicidal_dep.drop('All').drop('All', axis=1))
print(f"\nüìà Chi-square test:")
print(f"   œá¬≤ = {chi2_suicidal:.2f}, p = {p_suicidal:.4e}")

print(f"\nüìù K·∫æT LU·∫¨N:")
print(f"   - Suicidal Thoughts {'C√ì TH·ªÇ' if sensitivity > 0.5 and ppv > 0.5 else 'KH√ì'} d√πng l√†m t√≠n hi·ªáu c·∫£nh b√°o s·ªõm")
print(f"   - Sensitivity = {sensitivity*100:.1f}% (detect {sensitivity*100:.0f}% cases)")
print(f"   - PPV = {ppv*100:.1f}% (trong s·ªë c√≥ suicidal thoughts, {ppv*100:.0f}% th·ª±c s·ª± depression)")
print(f"   - Tuy nhi√™n, c·∫ßn k·∫øt h·ª£p v·ªõi c√°c y·∫øu t·ªë kh√°c ƒë·ªÉ c√≥ ƒë·ªô ch√≠nh x√°c cao h∆°n")
```

### Q14: Family History c√≤n √Ω nghƒ©a khi ƒë√£ ki·ªÉm so√°t m√¥i tr∆∞·ªùng s·ªëng?

```python
# ============================================
# CELL 24: Q14 - Family History controlling for environment
# ============================================

print("\n" + "-"*50)
print("Q14: Family History c√≤n √Ω nghƒ©a khi ƒë√£ ki·ªÉm so√°t m√¥i tr∆∞·ªùng s·ªëng?")
print("-"*50)

# 1. Unadjusted effect
fh_dep = df_clean.groupby('Family History of Mental Illness')['Depression'].mean() * 100
print("\nüìä T·ª∑ l·ªá tr·∫ßm c·∫£m theo Family History (unadjusted):")
for fh, rate in fh_dep.items():
    print(f"   {fh}: {rate:.1f}%")

# 2. Logistic Regression - Full model with environment variables
env_vars = ['Academic Pressure', 'Study Satisfaction', 'Financial Stress', 
            'Sleep_Hours_Encoded', 'Diet_Encoded', 'Work/Study Hours']

model_data_fh = df_clean[['Family_History', 'Depression'] + env_vars].dropna()

# Model 1: Family History only
X1 = sm.add_constant(model_data_fh['Family_History'])
y = model_data_fh['Depression']
model_fh_only = sm.Logit(y, X1).fit(disp=0)

# Model 2: Full model with environment
X2 = sm.add_constant(model_data_fh[['Family_History'] + env_vars])
model_full = sm.Logit(y, X2).fit(disp=0)

print("\nüìà Model 1: Family History Only")
print(f"   Coefficient: {model_fh_only.params['Family_History']:.4f}")
print(f"   OR: {np.exp(model_fh_only.params['Family_History']):.3f}")
print(f"   p-value: {model_fh_only.pvalues['Family_History']:.4e}")

print("\nüìà Model 2: Full Model (controlling for environment)")
print(f"   Family History Coefficient: {model_full.params['Family_History']:.4f}")
print(f"   OR: {np.exp(model_full.params['Family_History']):.3f}")
print(f"   p-value: {model_full.pvalues['Family_History']:.4e}")

# 3. Effect attenuation
attenuation = (1 - model_full.params['Family_History'] / model_fh_only.params['Family_History']) * 100
print(f"\nüìä Effect Attenuation: {attenuation:.1f}%")

# 4. Full model summary
print("\nüìà Full Model Coefficients:")
coef_df = pd.DataFrame({
    'Variable': model_full.params.index,
    'Coefficient': model_full.params.values,
    'OR': np.exp(model_full.params.values),
    'p-value': model_full.pvalues.values
})
display(coef_df[coef_df['Variable'] != 'const'].round(4))

# 5. Visualization
fig, axes = plt.subplots(1, 3, figsize=(16, 5))

# Bar chart - Depression by Family History
ax1 = axes[0]
colors = ['#2ecc71', '#e74c3c']
bars = ax1.bar(fh_dep.index, fh_dep.values, color=colors, edgecolor='black')
ax1.set_xlabel('Family History of Mental Illness')
ax1.set_ylabel('Depression Rate (%)')
ax1.set_title('Depression Rate by Family History (Unadjusted)', fontsize=11, fontweight='bold')
for bar in bars:
    height = bar.get_height()
    ax1.text(bar.get_x() + bar.get_width()/2., height + 1,
            f'{height:.1f}%', ha='center', fontsize=10)

# Coefficient comparison
ax2 = axes[1]
coef_comparison = pd.DataFrame({
    'Model': ['Family History Only', 'Full Model'],
    'Coefficient': [model_fh_only.params['Family_History'], model_full.params['Family_History']]
})
bars = ax2.bar(coef_comparison['Model'], coef_comparison['Coefficient'], 
               color=['steelblue', 'coral'], edgecolor='black')
ax2.set_ylabel('Coefficient (Log Odds)')
ax2.set_title('Family History Effect: Before vs After Adjustment', fontsize=11, fontweight='bold')
for bar in bars:
    height = bar.get_height()
    ax2.text(bar.get_x() + bar.get_width()/2., height + 0.01,
            f'{height:.3f}', ha='center', fontsize=10)

# Forest plot style - ORs
ax3 = axes[2]
vars_to_plot = ['Family_History', 'Academic Pressure', 'Financial Stress', 'Sleep_Hours_Encoded']
ors = [np.exp(model_full.params[v]) for v in vars_to_plot]
ci_low = [np.exp(model_full.conf_int().loc[v, 0]) for v in vars_to_plot]
ci_high = [np.exp(model_full.conf_int().loc[v, 1]) for v in vars_to_plot]

y_pos = range(len(vars_to_plot))
ax3.errorbar(ors, y_pos, xerr=[np.array(ors) - np.array(ci_low), np.array(ci_high) - np.array(ors)],
             fmt='o', capsize=5, markersize=8, color='navy')
ax3.axvline(1, color='red', linestyle='--', linewidth=2, label='OR = 1')
ax3.set_yticks(y_pos)
ax3.set_yticklabels(vars_to_plot)
ax3.set_xlabel('Odds Ratio (95% CI)')
ax3.set_title('Forest Plot: Adjusted Odds Ratios', fontsize=11, fontweight='bold')
ax3.legend()

plt.tight_layout()
plt.show()

print(f"\nüìù K·∫æT LU·∫¨N:")
if model_full.pvalues['Family_History'] < 0.05:
    print(f"   ‚úÖ Family History V·∫™N C√ì √ù NGHƒ®A sau khi ki·ªÉm so√°t m√¥i tr∆∞·ªùng")
    print(f"      (OR = {np.exp(model_full.params['Family_History']):.2f}, p = {model_full.pvalues['Family_History']:.4e})")
    print(f"   ‚û§ Y·∫øu t·ªë di truy·ªÅn/gia ƒë√¨nh c√≥ t√°c ƒë·ªông ƒê·ªòC L·∫¨P v·ªõi m√¥i tr∆∞·ªùng")
else:
    print(f"   ‚ùå Family History KH√îNG C√íN √ù NGHƒ®A sau khi ki·ªÉm so√°t m√¥i tr∆∞·ªùng")
    print(f"   ‚û§ T√°c ƒë·ªông c·ªßa Family History c√≥ th·ªÉ ƒë∆∞·ª£c gi·∫£i th√≠ch b·ªüi c√°c y·∫øu t·ªë m√¥i tr∆∞·ªùng")
```

---

## 3.6. T·ªîNG H·ª¢P & D·ª∞ B√ÅO

### Q15: X√¢y d·ª±ng m√¥ h√¨nh d·ª± b√°o tr·∫ßm c·∫£m

```python
# ============================================
# CELL 25: Q15 - Predictive Model
# ============================================

print("\n" + "="*70)
print("3.6 T·ªîNG H·ª¢P & D·ª∞ B√ÅO")
print("="*70)
print("\n" + "-"*50)
print("Q15: X√¢y d·ª±ng m√¥ h√¨nh d·ª± b√°o tr·∫ßm c·∫£m hi·ªáu qu·∫£ v√† di·ªÖn gi·∫£i ƒë∆∞·ª£c")
print("-"*50)

# 1. Feature Selection v√† Preparation
print("\nüìä Step 1: Feature Selection")

# Features to use
features = ['Age', 'Academic Pressure', 'Work Pressure', 'CGPA', 
            'Study Satisfaction', 'Job Satisfaction', 'Work/Study Hours',
            'Financial Stress', 'Sleep_Hours_Encoded', 'Diet_Encoded',
            'Suicidal_Thoughts', 'Family_History', 'Gender_Encoded']

# Prepare data
model_df = df_clean[features + ['Depression']].dropna()
print(f"   S·ªë l∆∞·ª£ng samples: {len(model_df):,}")
print(f"   S·ªë l∆∞·ª£ng features: {len(features)}")

X = model_df[features]
y = model_df['Depression']

# 2. Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, 
                                                     random_state=42, stratify=y)
print(f"\n   Train set: {len(X_train):,}")
print(f"   Test set: {len(X_test):,}")

# 3. Scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# 4. Train multiple models
print("\nüìà Step 2: Train Multiple Models")

models = {
    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),
    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),
    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),
    'Decision Tree': DecisionTreeClassifier(random_state=42, max_depth=5)
}

results = {}
for name, model in models.items():
    # Train
    if name == 'Logistic Regression':
        model.fit(X_train_scaled, y_train)
        y_pred = model.predict(X_test_scaled)
        y_prob = model.predict_proba(X_test_scaled)[:, 1]
    else:
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        y_prob = model.predict_proba(X_test)[:, 1]
    
    # Metrics
    acc = accuracy_score(y_test, y_pred)
    auc = roc_auc_score(y_test, y_prob)
    f1 = f1_score(y_test, y_pred)
    
    # Cross-validation
    if name == 'Logistic Regression':
        cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='roc_auc')
    else:
        cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='roc_auc')
    
    results[name] = {
        'Accuracy': acc,
        'AUC': auc,
        'F1': f1,
        'CV_AUC_mean': cv_scores.mean(),
        'CV_AUC_std': cv_scores.std(),
        'model': model,
        'y_prob': y_prob
    }
    
    print(f"\n   {name}:")
    print(f"      Accuracy: {acc:.4f}")
    print(f"      AUC: {auc:.4f}")
    print(f"      F1: {f1:.4f}")
    print(f"      CV AUC: {cv_scores.mean():.4f} (¬±{cv_scores.std():.4f})")

# 5. Model Comparison
print("\nüìä Step 3: Model Comparison")
comparison_df = pd.DataFrame({
    'Model': list(results.keys()),
    'Accuracy': [r['Accuracy'] for r in results.values()],
    'AUC': [r['AUC'] for r in results.values()],
    'F1 Score': [r['F1'] for r in results.values()],
    'CV AUC': [f"{r['CV_AUC_mean']:.4f}¬±{r['CV_AUC_std']:.4f}" for r in results.values()]
})
display(comparison_df)

# 6. Visualization
fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# ROC Curves
ax1 = axes[0, 0]
for name, res in results.items():
    fpr, tpr, _ = roc_curve(y_test, res['y_prob'])
    ax1.plot(fpr, tpr, label=f"{name} (AUC={res['AUC']:.3f})", linewidth=2)
ax1.plot([0, 1], [0, 1], 'k--', label='Random')
ax1.set_xlabel('False Positive Rate')
ax1.set_ylabel('True Positive Rate')
ax1.set_title('ROC Curves', fontsize=11, fontweight='bold')
ax1.legend(loc='lower right')
ax1.grid(True, alpha=0.3)

# Model comparison bar chart
ax2 = axes[0, 1]
x = np.arange(len(comparison_df))
width = 0.25
ax2.bar(x - width, comparison_df['Accuracy'], width, label='Accuracy', color='steelblue')
ax2.bar(x, comparison_df['AUC'], width, label='AUC', color='coral')
ax2.bar(x + width, comparison_df['F1 Score'], width, label='F1 Score', color='green')
ax2.set_xticks(x)
ax2.set_xticklabels(comparison_df['Model'], rotation=15, ha='right')
ax2.set_ylabel('Score')
ax2.set_title('Model Performance Comparison', fontsize=11, fontweight='bold')
ax2.legend()
ax2.set_ylim(0, 1)

# Best model - Confusion Matrix
best_model_name = max(results, key=lambda x: results[x]['AUC'])
best_model = results[best_model_name]['model']
if best_model_name == 'Logistic Regression':
    best_pred = best_model.predict(X_test_scaled)
else:
    best_pred = best_model.predict(X_test)

ax3 = axes[1, 0]
cm = confusion_matrix(y_test, best_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax3,
            xticklabels=['No Depression', 'Depression'],
            yticklabels=['No Depression', 'Depression'])
ax3.set_xlabel('Predicted')
ax3.set_ylabel('Actual')
ax3.set_title(f'Confusion Matrix - {best_model_name}', fontsize=11, fontweight='bold')

# Feature Importance (for best interpretable model)
ax4 = axes[1, 1]
if best_model_name == 'Logistic Regression':
    importance = np.abs(best_model.coef_[0])
else:
    importance = best_model.feature_importances_

importance_df = pd.DataFrame({
    'Feature': features,
    'Importance': importance
}).sort_values('Importance', ascending=True)

ax4.barh(importance_df['Feature'], importance_df['Importance'], color='teal', edgecolor='black')
ax4.set_xlabel('Importance')
ax4.set_title(f'Feature Importance - {best_model_name}', fontsize=11, fontweight='bold')

plt.tight_layout()
plt.show()

# 7. Classification Report
print(f"\nüìà Classification Report ({best_model_name}):")
print(classification_report(y_test, best_pred, target_names=['No Depression', 'Depression']))

# 8. Feature Importance Ranking
print("\nüìä Feature Importance Ranking:")
importance_df_sorted = importance_df.sort_values('Importance', ascending=False)
for i, (idx, row) in enumerate(importance_df_sorted.iterrows(), 1):
    print(f"   {i}. {row['Feature']}: {row['Importance']:.4f}")

# 9. Interpretability Analysis
print("\nüìù GI·∫¢I TH√çCH M√î H√åNH:")

# For Logistic Regression - show coefficients
if 'Logistic Regression' in results:
    lr_model = results['Logistic Regression']['model']
    coef_df = pd.DataFrame({
        'Feature': features,
        'Coefficient': lr_model.coef_[0],
        'Odds Ratio': np.exp(lr_model.coef_[0])
    }).sort_values('Coefficient', key=abs, ascending=False)
    
    print("\n   Logistic Regression Coefficients & Odds Ratios:")
    display(coef_df.round(4))
    
    print("\n   Top 5 Risk Factors (tƒÉng nguy c∆° tr·∫ßm c·∫£m):")
    risk_factors = coef_df[coef_df['Coefficient'] > 0].head(5)
    for i, (idx, row) in enumerate(risk_factors.iterrows(), 1):
        print(f"      {i}. {row['Feature']}: OR = {row['Odds Ratio']:.3f}")
    
    print("\n   Top 5 Protective Factors (gi·∫£m nguy c∆° tr·∫ßm c·∫£m):")
    protective = coef_df[coef_df['Coefficient'] < 0].head(5)
    for i, (idx, row) in enumerate(protective.iterrows(), 1):
        print(f"      {i}. {row['Feature']}: OR = {row['Odds Ratio']:.3f}")

print(f"\nüìù K·∫æT LU·∫¨N:")
print(f"   - Best Model: {best_model_name}")
print(f"   - Test AUC: {results[best_model_name]['AUC']:.4f}")
print(f"   - C√≥ th·ªÉ x√¢y d·ª±ng m√¥ h√¨nh d·ª± b√°o v·ªõi hi·ªáu qu·∫£ {'T·ªêT' if results[best_model_name]['AUC'] > 0.75 else 'TRUNG B√åNH'}")
print(f"   - M√¥ h√¨nh c√≥ th·ªÉ ƒë∆∞·ª£c s·ª≠ d·ª•ng nh∆∞ c√¥ng c·ª• s√†ng l·ªçc s∆° b·ªô")
```

---

# 4. RECOMMENDED RESEARCH QUESTIONS B·ªî SUNG

```python
# ============================================
# CELL 26: Additional Research Questions
# ============================================

print("\n" + "="*70)
print("4. RECOMMENDED RESEARCH QUESTIONS B·ªî SUNG")
print("="*70)

additional_rqs = """
## 4.1 Recommended Research Questions

D·ª±a tr√™n ph√¢n t√≠ch d·ªØ li·ªáu, ƒë√¢y l√† m·ªôt s·ªë c√¢u h·ªèi nghi√™n c·ª©u th√∫ v·ªã v√† ƒë·∫∑c s·∫Øc m√† b·∫°n c√≥ th·ªÉ kh√°m ph√° th√™m:

### üî¨ RQ1: Ph√¢n t√≠ch Cluster - C√≥ t·ªìn t·∫°i c√°c "profile" sinh vi√™n c√≥ nguy c∆° cao?
**M·ª•c ti√™u:** S·ª≠ d·ª•ng K-means ho·∫∑c hierarchical clustering ƒë·ªÉ ph√°t hi·ªán c√°c nh√≥m sinh vi√™n 
c√≥ ƒë·∫∑c ƒëi·ªÉm t∆∞∆°ng t·ª± nhau, t·ª´ ƒë√≥ x√°c ƒë·ªãnh c√°c "profile" c√≥ nguy c∆° tr·∫ßm c·∫£m cao.

**Ph∆∞∆°ng ph√°p:**
- K-means clustering v·ªõi c√°c bi·∫øn: Academic Pressure, Study Satisfaction, Sleep, Financial Stress
- Elbow method ƒë·ªÉ ch·ªçn s·ªë cluster
- Ph√¢n t√≠ch ƒë·∫∑c ƒëi·ªÉm t·ª´ng cluster

### üî¨ RQ2: Interaction Effects ph·ª©c t·∫°p - Sleep √ó Financial Stress √ó Academic Pressure
**M·ª•c ti√™u:** Ki·ªÉm tra xem ba y·∫øu t·ªë n√†y c√≥ t∆∞∆°ng t√°c ph·ª©c t·∫°p v·ªõi nhau kh√¥ng.

**Gi·∫£ thuy·∫øt:** 
- Thi·∫øu ng·ªß + Stress t√†i ch√≠nh + √Åp l·ª±c h·ªçc cao = "Perfect storm" cho tr·∫ßm c·∫£m
- T·ªï h·ª£p n√†y nguy hi·ªÉm h∆°n t·ªïng c·ªßa t·ª´ng y·∫øu t·ªë ri√™ng l·∫ª

### üî¨ RQ3: Geographic Disparities - S·ª± kh√°c bi·ªát gi·ªØa c√°c th√†nh ph·ªë
**M·ª•c ti√™u:** Ph√¢n t√≠ch xem v·ªã tr√≠ ƒë·ªãa l√Ω c√≥ ·∫£nh h∆∞·ªüng ƒë·∫øn t·ª∑ l·ªá tr·∫ßm c·∫£m kh√¥ng.

**Ph∆∞∆°ng ph√°p:**
- So s√°nh t·ª∑ l·ªá tr·∫ßm c·∫£m gi·ªØa c√°c th√†nh ph·ªë
- Multilevel modeling v·ªõi City nh∆∞ random effect
- Ki·ªÉm tra xem c√°c y·∫øu t·ªë r·ªßi ro c√≥ kh√°c nhau gi·ªØa c√°c th√†nh ph·ªë kh√¥ng

### üî¨ RQ4: Degree-specific Analysis
**M·ª•c ti√™u:** C√°c ng√†nh h·ªçc/b·∫±ng c·∫•p n√†o c√≥ nguy c∆° cao nh·∫•t?

**Ph∆∞∆°ng ph√°p:**
- So s√°nh Depression rate theo Degree
- Ki·ªÉm tra xem Academic Pressure c√≥ kh√°c nhau gi·ªØa c√°c ng√†nh kh√¥ng
- Ph√¢n t√≠ch xem m·ªëi quan h·ªá AP-Depression c√≥ kh√°c nhau theo ng√†nh kh√¥ng

### üî¨ RQ5: Non-linear Effects v·ªõi Spline Regression
**M·ª•c ti√™u:** C√°c bi·∫øn li√™n t·ª•c (Age, CGPA, Work Hours) c√≥ t√°c ƒë·ªông phi tuy·∫øn ph·ª©c t·∫°p?

**Ph∆∞∆°ng ph√°p:**
- Restricted cubic splines
- GAM (Generalized Additive Models)
- X√°c ƒë·ªãnh c√°c "tipping points" cho t·ª´ng bi·∫øn

### üî¨ RQ6: Causal Inference v·ªõi Propensity Score Matching
**M·ª•c ti√™u:** ∆Ø·ªõc l∆∞·ª£ng t√°c ƒë·ªông "nh√¢n qu·∫£" c·ªßa Academic Pressure l√™n Depression

**Ph∆∞∆°ng ph√°p:**
- Propensity score matching
- Inverse probability weighting
- So s√°nh v·ªõi regression adjustment

### üî¨ RQ7: Machine Learning Interpretability - SHAP Analysis
**M·ª•c ti√™u:** Gi·∫£i th√≠ch d·ª± ƒëo√°n c·ªßa m√¥ h√¨nh ·ªü m·ª©c c√° nh√¢n

**Ph∆∞∆°ng ph√°p:**
- SHAP values cho Random Forest/Gradient Boosting
- Feature interactions
- Counterfactual explanations

### üî¨ RQ8: Time-to-Depression Risk Score
**M·ª•c ti√™u:** X√¢y d·ª±ng risk score c√≥ th·ªÉ s·ª≠ d·ª•ng trong th·ª±c t·∫ø

**Ph∆∞∆°ng ph√°p:**
- Logistic regression v·ªõi t√≠nh ƒëi·ªÉm ƒë∆°n gi·∫£n
- Nomogram visualization
- Risk stratification (Low/Medium/High)
"""

print(additional_rqs)

# Code skeleton for RQ1 (Cluster Analysis)
print("\n" + "-"*50)
print("üìù CODE SKELETON cho RQ1 (Cluster Analysis):")
print("-"*50)

cluster_code = '''
# Cluster Analysis - Profile Detection
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

# Select features for clustering
cluster_features = ['Academic Pressure', 'Study Satisfaction', 
                   'Sleep_Hours_Encoded', 'Financial Stress', 'Work/Study Hours']

# Prepare data
X_cluster = df_clean[cluster_features].dropna()
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_cluster)

# Elbow method
inertias = []
K_range = range(2, 10)
for k in K_range:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    kmeans.fit(X_scaled)
    inertias.append(kmeans.inertia_)

plt.plot(K_range, inertias, 'bo-')
plt.xlabel('Number of Clusters (k)')
plt.ylabel('Inertia')
plt.title('Elbow Method')
plt.show()

# Fit with optimal k
optimal_k = 4  # Based on elbow
kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)
clusters = kmeans.fit_predict(X_scaled)

# Add to dataframe and analyze
df_cluster = df_clean.loc[X_cluster.index].copy()
df_cluster['Cluster'] = clusters

# Cluster profiles
cluster_profiles = df_cluster.groupby('Cluster')[cluster_features + ['Depression']].mean()
print(cluster_profiles)

# Depression rate by cluster
cluster_dep = df_cluster.groupby('Cluster')['Depression'].mean() * 100
print("Depression Rate by Cluster:")
print(cluster_dep)
'''
print(cluster_code)
```

```python
# ============================================
# CELL 27: Final Summary
# ============================================

print("\n" + "="*70)
print("üìã FINAL SUMMARY")
print("="*70)

summary = """
## T√≥m t·∫Øt c√°c ph√°t hi·ªán ch√≠nh

### 1. T·ª∑ l·ªá tr·∫ßm c·∫£m
- T·ª∑ l·ªá tr·∫ßm c·∫£m trong sinh vi√™n kh√° cao (~50%)
- Kh√¥ng c√≥ s·ª± kh√°c bi·ªát ƒë√°ng k·ªÉ gi·ªØa nam v√† n·ªØ

### 2. Y·∫øu t·ªë r·ªßi ro m·∫°nh nh·∫•t
1. **Academic Pressure** - Y·∫øu t·ªë m·∫°nh nh·∫•t
2. **Suicidal Thoughts** - T√≠n hi·ªáu c·∫£nh b√°o quan tr·ªçng
3. **Financial Stress** - ·∫¢nh h∆∞·ªüng ƒë·ªôc l·∫≠p
4. **Sleep Duration** - Thi·∫øu ng·ªß tƒÉng nguy c∆° ƒë√°ng k·ªÉ

### 3. Y·∫øu t·ªë b·∫£o v·ªá
1. **Study Satisfaction** - Gi·∫£m nguy c∆°
2. **Healthy Dietary Habits** - C√≥ t√°c ƒë·ªông t√≠ch c·ª±c
3. **Adequate Sleep (7-8 hours)** - B·∫£o v·ªá s·ª©c kh·ªèe tinh th·∫ßn

### 4. Insights ƒë√°ng ch√∫ √Ω
- "High achievers but unhappy" c√≥ t·ªìn t·∫°i
- Academic Pressure v√† Financial Stress ƒë·ªÅu quan tr·ªçng, nh∆∞ng AP m·∫°nh h∆°n
- Family History v·∫´n c√≥ √Ω nghƒ©a sau khi ki·ªÉm so√°t c√°c y·∫øu t·ªë m√¥i tr∆∞·ªùng
- Sleep c√≥ t√°c ƒë·ªông c·∫£ tr·ª±c ti·∫øp v√† gi√°n ti·∫øp

### 5. M√¥ h√¨nh d·ª± b√°o
- C√≥ th·ªÉ x√¢y d·ª±ng m√¥ h√¨nh v·ªõi AUC > 0.75
- Top features: Academic Pressure, Suicidal Thoughts, Study Satisfaction, Sleep

### 6. Recommendations
- Can thi·ªáp s·ªõm cho sinh vi√™n c√≥ Academic Pressure cao
- Ch√∫ √Ω ƒë·∫∑c bi·ªát ƒë·∫øn sinh vi√™n c√≥ suicidal thoughts
- Ch∆∞∆°ng tr√¨nh h·ªó tr·ª£ t√†i ch√≠nh v√† qu·∫£n l√Ω stress
- Gi√°o d·ª•c v·ªÅ t·∫ßm quan tr·ªçng c·ªßa gi·∫•c ng·ªß v√† l·ªëi s·ªëng l√†nh m·∫°nh
"""

print(summary)
```

---

## H∆Ø·ªöNG D·∫™N S·ª¨ D·ª§NG NOTEBOOK

1. **Ch·∫°y tu·∫ßn t·ª± t·ª´ ƒë·∫ßu ƒë·∫øn cu·ªëi** - C√°c cell ph·ª• thu·ªôc v√†o nhau
2. **ƒê·ªçc k·ªπ output v√† interpretation** - M·ªói ph√¢n t√≠ch ƒë·ªÅu c√≥ gi·∫£i th√≠ch
3. **Customize theo nhu c·∫ßu** - C√≥ th·ªÉ thay ƒë·ªïi c√°c threshold, bins, etc.
4. **Th√™m visualizations** n·∫øu c·∫ßn thi·∫øt
5. **Ki·ªÉm tra c√°c assumptions** c·ªßa statistical tests

## L∆ØU √ù QUAN TR·ªåNG

- D·ªØ li·ªáu t·ª± b√°o c√°o c√≥ th·ªÉ c√≥ bias
- Kh√¥ng n√™n t·ªïng qu√°t h√≥a qu√° m·ª©c k·∫øt qu·∫£
- ƒê√¢y l√† ph√¢n t√≠ch cross-sectional, kh√¥ng th·ªÉ k·∫øt lu·∫≠n nh√¢n qu·∫£
- C·∫ßn validation v·ªõi d·ªØ li·ªáu kh√°c tr∆∞·ªõc khi √°p d·ª•ng th·ª±c t·∫ø
